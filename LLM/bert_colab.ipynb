{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import math\n",
    "from google.colab import drive"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List of models you want to use\n",
    "model_name = 'unitary/toxic-bert'"
   ],
   "id": "bce56eba1dab310b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to dynamically split text into equally sized chunks and sum the toxicity scores\n",
    "def classify_toxicity_by_dynamic_chunks(text, tokenizer, classifier, max_length=512):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "\n",
    "    # Get total number of tokens\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    total_tokens = len(input_ids)\n",
    "\n",
    "    # Calculate chunk size based on the total number of tokens\n",
    "    if total_tokens <= max_length:\n",
    "        num_chunks = 1\n",
    "        chunk_size = total_tokens\n",
    "    else:\n",
    "        num_chunks = math.ceil(total_tokens / max_length)\n",
    "        chunk_size = math.ceil(total_tokens / num_chunks)\n",
    "        # chunk_size = max_length\n",
    "\n",
    "    # Initialize list to store toxicity scores\n",
    "    toxicity_scores = []\n",
    "\n",
    "    # Iterate through each dynamically sized chunk\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, total_tokens)\n",
    "\n",
    "        chunk = tokenizer.decode(input_ids[start:end], skip_special_tokens=True)\n",
    "\n",
    "        # Get the toxicity scores for all labels in the chunk\n",
    "        result = classifier(chunk)\n",
    "\n",
    "        # Sum scores from all labels\n",
    "        score_sum = sum(res['score'] for res in result[0])  # Summing all label scores\n",
    "        toxicity_scores.append(score_sum)\n",
    "\n",
    "    # Return the average of the summed scores across all chunks\n",
    "    return sum(toxicity_scores) / len(toxicity_scores) if toxicity_scores else 0"
   ],
   "id": "1065ccd345fed880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to classify toxicity scores into 'toxic' or 'non-toxic' labels\n",
    "def classify_toxicity_score(score, threshold):\n",
    "    return \"toxic\" if score > threshold else \"non-toxic\""
   ],
   "id": "76cd0ebdd91454b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "drive.mount('/content/drive')\n",
    "file_path = '/content/drive/My Drive/Colab Notebooks/cleaned_data_2223.csv'\n",
    "df = pd.read_csv(file_path)"
   ],
   "id": "57ae0bea1077080b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Part 1: Running on sample.xlsx (with comment matching)Ã¥\n",
    "start_index=0\n",
    "# end_index=1600000\n",
    "df_sample = df[start_index:]\n",
    "df_sample['predicted_label'] = None\n",
    "df_sample"
   ],
   "id": "f53b9861701f72c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "singlish_toxic_dict = ['ahbeng', 'ahlian', 'baka', 'bloody hell', 'bloody idiot', 'bodoh', 'bo liao',\n",
    "                       'buay pai seh', 'buay tahan', 'cb', 'cb kia', 'cb knn', 'cb', 'cb lao jia', 'cb lao knn',\n",
    "                       'cb lor', 'cb sia', 'cb sia kia', 'ccb', 'chbye kia', 'chao chbye', 'chao chee bye', 'chow chibai',\n",
    "                       'chow kar', 'chow tu lan', 'cibai', 'dumb ass', 'dumb', 'fuck', 'fuck you', 'fking', 'fucker',\n",
    "                       'fucker sia', 'gila babi', 'gundu', 'hao lian kia', 'hopeless', 'idiot', 'idiot', 'ji bai', 'jiat lat',\n",
    "                       'jialat kia', 'jibai', 'joker', 'kan', 'kan ni na', 'kena sai', 'kia si lang', 'knn', 'knn cb kia', 'knnccb',\n",
    "                       'knnbccb', 'kns', 'kns cb', 'lampa', 'lan pa', 'lanjiao', 'lanjiao kia', 'lj', 'loser', 'nabei', 'no use kia',\n",
    "                       'noob', 'pok gai', 'pui', 'sabo kia', 'sibei jialat', 'sibei sian', 'si gina', 'siol', 'slut', 'siao lang', 'stupid',\n",
    "                       'suck', 'sua gu', 'tmd', 'tiok knn', 'tiok tiam', 'useless', 'what knn', 'what the fuck', 'wtf', 'wu liao kia', 'you die ah', 'you die']"
   ],
   "id": "33db86ffc0922712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the pipeline\n",
    "classifier = pipeline('text-classification', model=model_name, top_k=None, device=0, batch_size=8)\n",
    "\n",
    "labels = []\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in df_sample.iterrows():\n",
    "    text = row['text']\n",
    "\n",
    "    if any(word in text for word in singlish_toxic_dict):\n",
    "        predicted_label = \"toxic\"\n",
    "\n",
    "    else:\n",
    "        # Get toxicity score for the text\n",
    "        toxicity_score = classify_toxicity_by_dynamic_chunks(text, tokenizer, classifier)\n",
    "\n",
    "        # Classify the toxicity score based on the threshold\n",
    "        predicted_label = classify_toxicity_score(toxicity_score, threshold=0.1)\n",
    "\n",
    "    df_sample.at[index, 'predicted_label'] = predicted_label\n",
    "\n",
    "# Save the updated DataFrame with labels to a new CSV file\n",
    "output_file_path = '/content/drive/My Drive/Colab Notebooks/cleaned_data_2223_labelled' + str(start_index) + '-' + '.csv'\n",
    "df_sample.to_csv(output_file_path)"
   ],
   "id": "bcc166f92461ac89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_temp = df_sample[(df_sample['predicted_label'] == 'toxic') | (df_sample['predicted_label'] == \"non-toxic\")]\n",
    "df_temp\n",
    "output_file_path = '/content/drive/My Drive/Colab Notebooks/cleaned_data_2223_labelled_0-308212.csv'\n",
    "df_temp.to_csv(output_file_path)"
   ],
   "id": "b626abd80359a034"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
