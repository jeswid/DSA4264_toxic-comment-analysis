{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>link</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>moderation</th>\n",
       "      <th>original text</th>\n",
       "      <th>manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>972545</td>\n",
       "      <td>1057760</td>\n",
       "      <td>americans hold beer oh wait ive finished drinking</td>\n",
       "      <td>2022-01-24 05:25:27</td>\n",
       "      <td>MrFantasticallyNerdy</td>\n",
       "      <td>/r/SingaporeRaw/comments/sao76t/we_are_spendin...</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>htzg3uf</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>Americans: Hold my beer. Oh wait. I've already...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13347</td>\n",
       "      <td>14500</td>\n",
       "      <td>boss relies heavily pass holders start asking ...</td>\n",
       "      <td>2022-05-06 10:50:15</td>\n",
       "      <td>SimpleReadingSG90</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...</td>\n",
       "      <td>t3_ujhtgu</td>\n",
       "      <td>t1_i7j8gnf</td>\n",
       "      <td>i7jextx</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>And the boss who relies heavily on S Pass hold...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>461901</td>\n",
       "      <td>502383</td>\n",
       "      <td>mean youre paid much wipe youre tears money ea...</td>\n",
       "      <td>2022-05-08 05:02:44</td>\n",
       "      <td>OldFee3150</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujddhe/chan_chun_sing...</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>i7rioq7</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>I mean if youâ€™re being paid that much should j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1094356</td>\n",
       "      <td>1190080</td>\n",
       "      <td>lol idea hes singing singapore practice sang w...</td>\n",
       "      <td>2022-09-27 06:09:30</td>\n",
       "      <td>kopisiutaidaily</td>\n",
       "      <td>/r/SingaporeRaw/comments/xonblv/communist_taxi...</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>iq2n4xp</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>Lol he has no idea what heâ€™s singing. If Singa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1153986</td>\n",
       "      <td>1254859</td>\n",
       "      <td>would probably change sgkl railway line opens</td>\n",
       "      <td>2023-08-09 04:33:32</td>\n",
       "      <td>TaylorFritz</td>\n",
       "      <td>/r/SingaporeRaw/comments/15m2a52/why_do_singap...</td>\n",
       "      <td>t3_15m2a52</td>\n",
       "      <td>t1_jveg4qj</td>\n",
       "      <td>jveglrl</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>That would probably change once the SG-KL rail...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>847882</td>\n",
       "      <td>922205</td>\n",
       "      <td>yo thats night snack muffin ate last week haha...</td>\n",
       "      <td>2023-05-08 02:57:20</td>\n",
       "      <td>Rogueone65</td>\n",
       "      <td>/r/singapore/comments/13b8w1z/sq207_sin_to_mel...</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>jjaiifo</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Yo thatâ€™s the night snack muffin I ate last we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1377594</td>\n",
       "      <td>1498060</td>\n",
       "      <td>hello thank posting singapore seems previously...</td>\n",
       "      <td>2023-08-21 16:01:39</td>\n",
       "      <td>ModeratelyUsefulBot</td>\n",
       "      <td>/r/singapore/comments/15x9vq1/public_hospitals...</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>jx5avr9</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Hello and thank you for posting to singapore! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1498756</td>\n",
       "      <td>1629859</td>\n",
       "      <td>guys celebrate vesak day too</td>\n",
       "      <td>2023-05-26 22:18:28</td>\n",
       "      <td>brownriver12</td>\n",
       "      <td>/r/singapore/comments/13spsi3/rsingapore_rando...</td>\n",
       "      <td>t3_13spsi3</td>\n",
       "      <td>t1_jlr58id</td>\n",
       "      <td>jlr5ecb</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>You guys celebrate vesak day too?ðŸ¤£</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1312718</td>\n",
       "      <td>1427653</td>\n",
       "      <td>stand upon shoulders giants walked may run ins...</td>\n",
       "      <td>2022-08-24 04:03:51</td>\n",
       "      <td>fish312</td>\n",
       "      <td>/r/singapore/comments/ww82t0/from_aug_29_masks...</td>\n",
       "      <td>t3_ww82t0</td>\n",
       "      <td>t1_iljpegw</td>\n",
       "      <td>iljvxm7</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>We stand upon the shoulders of giants. She wal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>655848</td>\n",
       "      <td>713362</td>\n",
       "      <td>oh crap skimmed need tattoo removal services d...</td>\n",
       "      <td>2023-04-29 03:14:22</td>\n",
       "      <td>Turbografx220</td>\n",
       "      <td>/r/singapore/comments/132h8jg/tattoo_removals_...</td>\n",
       "      <td>t3_132h8jg</td>\n",
       "      <td>t1_ji53apm</td>\n",
       "      <td>ji53onk</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Oh crap I skimmed it.\\n\\nWe need more of tatto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0          972545     1057760   \n",
       "1           13347       14500   \n",
       "2          461901      502383   \n",
       "3         1094356     1190080   \n",
       "4         1153986     1254859   \n",
       "..            ...         ...   \n",
       "195        847882      922205   \n",
       "196       1377594     1498060   \n",
       "197       1498756     1629859   \n",
       "198       1312718     1427653   \n",
       "199        655848      713362   \n",
       "\n",
       "                                                  text            timestamp  \\\n",
       "0    americans hold beer oh wait ive finished drinking  2022-01-24 05:25:27   \n",
       "1    boss relies heavily pass holders start asking ...  2022-05-06 10:50:15   \n",
       "2    mean youre paid much wipe youre tears money ea...  2022-05-08 05:02:44   \n",
       "3    lol idea hes singing singapore practice sang w...  2022-09-27 06:09:30   \n",
       "4        would probably change sgkl railway line opens  2023-08-09 04:33:32   \n",
       "..                                                 ...                  ...   \n",
       "195  yo thats night snack muffin ate last week haha...  2023-05-08 02:57:20   \n",
       "196  hello thank posting singapore seems previously...  2023-08-21 16:01:39   \n",
       "197                       guys celebrate vesak day too  2023-05-26 22:18:28   \n",
       "198  stand upon shoulders giants walked may run ins...  2022-08-24 04:03:51   \n",
       "199  oh crap skimmed need tattoo removal services d...  2023-04-29 03:14:22   \n",
       "\n",
       "                 username                                               link  \\\n",
       "0    MrFantasticallyNerdy  /r/SingaporeRaw/comments/sao76t/we_are_spendin...   \n",
       "1       SimpleReadingSG90  /r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...   \n",
       "2              OldFee3150  /r/SingaporeRaw/comments/ujddhe/chan_chun_sing...   \n",
       "3         kopisiutaidaily  /r/SingaporeRaw/comments/xonblv/communist_taxi...   \n",
       "4             TaylorFritz  /r/SingaporeRaw/comments/15m2a52/why_do_singap...   \n",
       "..                    ...                                                ...   \n",
       "195            Rogueone65  /r/singapore/comments/13b8w1z/sq207_sin_to_mel...   \n",
       "196   ModeratelyUsefulBot  /r/singapore/comments/15x9vq1/public_hospitals...   \n",
       "197          brownriver12  /r/singapore/comments/13spsi3/rsingapore_rando...   \n",
       "198               fish312  /r/singapore/comments/ww82t0/from_aug_29_masks...   \n",
       "199         Turbografx220  /r/singapore/comments/132h8jg/tattoo_removals_...   \n",
       "\n",
       "        link_id   parent_id       id subreddit_id  \\\n",
       "0     t3_sao76t   t3_sao76t  htzg3uf     t5_xnx04   \n",
       "1     t3_ujhtgu  t1_i7j8gnf  i7jextx     t5_xnx04   \n",
       "2     t3_ujddhe   t3_ujddhe  i7rioq7     t5_xnx04   \n",
       "3     t3_xonblv   t3_xonblv  iq2n4xp     t5_xnx04   \n",
       "4    t3_15m2a52  t1_jveg4qj  jveglrl     t5_xnx04   \n",
       "..          ...         ...      ...          ...   \n",
       "195  t3_13b8w1z  t3_13b8w1z  jjaiifo     t5_2qh8c   \n",
       "196  t3_15x9vq1  t3_15x9vq1  jx5avr9     t5_2qh8c   \n",
       "197  t3_13spsi3  t1_jlr58id  jlr5ecb     t5_2qh8c   \n",
       "198   t3_ww82t0  t1_iljpegw  iljvxm7     t5_2qh8c   \n",
       "199  t3_132h8jg  t1_ji53apm  ji53onk     t5_2qh8c   \n",
       "\n",
       "                                            moderation  \\\n",
       "0    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "1    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "2    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "3    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "4    {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "..                                                 ...   \n",
       "195  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "196  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "197  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "198  {'controversiality': 0, 'collapsed_reason_code...   \n",
       "199  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "\n",
       "                                         original text  manual  \n",
       "0    Americans: Hold my beer. Oh wait. I've already...       0  \n",
       "1    And the boss who relies heavily on S Pass hold...       0  \n",
       "2    I mean if youâ€™re being paid that much should j...       0  \n",
       "3    Lol he has no idea what heâ€™s singing. If Singa...       0  \n",
       "4    That would probably change once the SG-KL rail...       0  \n",
       "..                                                 ...     ...  \n",
       "195  Yo thatâ€™s the night snack muffin I ate last we...       0  \n",
       "196  Hello and thank you for posting to singapore! ...       0  \n",
       "197                 You guys celebrate vesak day too?ðŸ¤£       0  \n",
       "198  We stand upon the shoulders of giants. She wal...       0  \n",
       "199  Oh crap I skimmed it.\\n\\nWe need more of tatto...       0  \n",
       "\n",
       "[200 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled = pd.read_excel('manual_label_sample.xlsx')\n",
    "labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# Author: C.J. Hutto\n",
    "# Thanks to George Berry for reducing the time complexity from something like O(N^4) to O(N).\n",
    "# Thanks to Ewan Klein and Pierpaolo Pantone for bringing VADER into NLTK. Those modifications were awesome.\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\"\"\"\n",
    "If you use the VADER sentiment analysis tools, please cite:\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "import codecs\n",
    "import json\n",
    "from itertools import product\n",
    "from inspect import getsourcefile\n",
    "from io import open\n",
    "\n",
    "# ##Constants##\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "# (empirically derived mean sentiment intensity rating increase for using ALLCAPs to emphasize a word)\n",
    "C_INCR = 0.733\n",
    "N_SCALAR = -0.74\n",
    "\n",
    "NEGATE = \\\n",
    "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
    "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
    "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
    "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
    "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
    "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
    "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
    "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
    "\n",
    "# booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "BOOSTER_DICT = \\\n",
    "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR,\n",
    "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
    "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
    "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR,\n",
    "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
    "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
    "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR,\n",
    "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
    "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR,\n",
    "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR,\n",
    "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
    "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
    "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
    "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
    "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
    "     \"very\": B_INCR,\n",
    "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
    "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
    "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
    "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
    "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
    "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n",
    "\n",
    "# check for sentiment laden idioms that do not contain lexicon words (future work, not yet implemented)\n",
    "SENTIMENT_LADEN_IDIOMS = {\"cut the mustard\": 2, \"hand to mouth\": -2,\n",
    "                          \"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                          \"upper hand\": 1, \"break a leg\": 2,\n",
    "                          \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                          \"on the ball\": 2, \"under the weather\": -2}\n",
    "\n",
    "# check for special case idioms and phrases containing lexicon words\n",
    "SPECIAL_CASES = {\"the shit\": 3, \"the bomb\": 3, \"bad ass\": 1.5, \"badass\": 1.5, \"bus stop\": 0.0,\n",
    "                 \"yeah right\": -2, \"kiss of death\": -1.5, \"to die for\": 3,\n",
    "                 \"beating heart\": 3.1, \"broken heart\": -2.9 }\n",
    "\n",
    "\n",
    "# #Static methods# #\n",
    "\n",
    "def negated(input_words, include_nt=True):\n",
    "    \"\"\"\n",
    "    Determine if input contains negation words\n",
    "    \"\"\"\n",
    "    input_words = [str(w).lower() for w in input_words]\n",
    "    neg_words = []\n",
    "    neg_words.extend(NEGATE)\n",
    "    for word in neg_words:\n",
    "        if word in input_words:\n",
    "            return True\n",
    "    if include_nt:\n",
    "        for word in input_words:\n",
    "            if \"n't\" in word:\n",
    "                return True\n",
    "    '''if \"least\" in input_words:\n",
    "        i = input_words.index(\"least\")\n",
    "        if i > 0 and input_words[i - 1] != \"at\":\n",
    "            return True'''\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score / math.sqrt((score * score) + alpha)\n",
    "    if norm_score < -1.0:\n",
    "        return -1.0\n",
    "    elif norm_score > 1.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return norm_score\n",
    "\n",
    "\n",
    "def allcap_differential(words):\n",
    "    \"\"\"\n",
    "    Check whether just some words in the input are ALL CAPS\n",
    "    :param list words: The words to inspect\n",
    "    :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "    \"\"\"\n",
    "    is_different = False\n",
    "    allcap_words = 0\n",
    "    for word in words:\n",
    "        if word.isupper():\n",
    "            allcap_words += 1\n",
    "    cap_differential = len(words) - allcap_words\n",
    "    if 0 < cap_differential < len(words):\n",
    "        is_different = True\n",
    "    return is_different\n",
    "\n",
    "\n",
    "def scalar_inc_dec(word, valence, is_cap_diff):\n",
    "    \"\"\"\n",
    "    Check if the preceding words increase, decrease, or negate/nullify the\n",
    "    valence\n",
    "    \"\"\"\n",
    "    scalar = 0.0\n",
    "    word_lower = word.lower()\n",
    "    if word_lower in BOOSTER_DICT:\n",
    "        scalar = BOOSTER_DICT[word_lower]\n",
    "        if valence < 0:\n",
    "            scalar *= -1\n",
    "        # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "        if word.isupper() and is_cap_diff:\n",
    "            if valence > 0:\n",
    "                scalar += C_INCR\n",
    "            else:\n",
    "                scalar -= C_INCR\n",
    "    return scalar\n",
    "\n",
    "\n",
    "class SentiText(object):\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text).encode('utf-8')\n",
    "        self.text = text\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\\\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_punc_if_word(token):\n",
    "        \"\"\"\n",
    "        Removes all trailing and leading punctuation\n",
    "        If the resulting string has two or fewer characters,\n",
    "        then it was likely an emoticon, so return original string\n",
    "        (ie \":)\" stripped would be \"\", so just return \":)\"\n",
    "        \"\"\"\n",
    "        stripped = token.strip(string.punctuation)\n",
    "        if len(stripped) <= 2:\n",
    "            return token\n",
    "        return stripped\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        stripped = list(map(self._strip_punc_if_word, wes))\n",
    "        return stripped\n",
    "\n",
    "class SentimentIntensityAnalyzer(object):\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lexicon_file=\"vader_lexicon.txt\", emoji_lexicon=\"emoji_utf8_lexicon.txt\"):\n",
    "        lexicon_full_filepath = lexicon_file\n",
    "        with codecs.open(lexicon_full_filepath, encoding='utf-8') as f:\n",
    "            self.lexicon_full_filepath = f.read()\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "\n",
    "        emoji_full_filepath = emoji_lexicon\n",
    "        with codecs.open(emoji_full_filepath, encoding='utf-8') as f:\n",
    "            self.emoji_full_filepath = f.read()\n",
    "        self.emojis = self.make_emoji_dict()\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "            (word, measure) = line.strip().split('\\t')[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "    def make_emoji_dict(self):\n",
    "        \"\"\"\n",
    "        Convert emoji lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        emoji_dict = {}\n",
    "        for line in self.emoji_full_filepath.rstrip('\\n').split('\\n'):\n",
    "            (emoji, description) = line.strip().split('\\t')[0:2]\n",
    "            emoji_dict[emoji] = description\n",
    "        return emoji_dict\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "        \"\"\"\n",
    "        # convert emojis to their textual descriptions\n",
    "        text_no_emoji = \"\"\n",
    "        prev_space = True\n",
    "        for chr in text:\n",
    "            if chr in self.emojis:\n",
    "                # get the textual description\n",
    "                description = self.emojis[chr]\n",
    "                if not prev_space:\n",
    "                    text_no_emoji += ' '\n",
    "                text_no_emoji += description\n",
    "                prev_space = False\n",
    "            else:\n",
    "                text_no_emoji += chr\n",
    "                prev_space = chr == ' '\n",
    "        text = text_no_emoji.strip()\n",
    "\n",
    "        sentitext = SentiText(text)\n",
    "\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for i, item in enumerate(words_and_emoticons):\n",
    "            valence = 0\n",
    "            # check for vader_lexicon words that may be used as modifiers or negations\n",
    "            if item.lower() in BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "            if (i < len(words_and_emoticons) - 1 and item.lower() == \"kind\" and\n",
    "                    words_and_emoticons[i + 1].lower() == \"of\"):\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        valence_dict = self.score_valence(sentiments, text)\n",
    "\n",
    "        return valence_dict\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence \n",
    "            valence = self.lexicon[item_lowercase]\n",
    "\n",
    "            # check for \"no\" as negation for an adjacent lexicon item vs \"no\" as its own stand-alone lexicon item\n",
    "            if item_lowercase == \"no\" and i != len(words_and_emoticons)-1 and words_and_emoticons[i + 1].lower() in self.lexicon:\n",
    "                # don't use valence of \"no\" as a lexicon item. Instead set it's valence to 0.0 and negate the next item\n",
    "                valence = 0.0\n",
    "            if (i > 0 and words_and_emoticons[i - 1].lower() == \"no\") \\\n",
    "               or (i > 1 and words_and_emoticons[i - 2].lower() == \"no\") \\\n",
    "               or (i > 2 and words_and_emoticons[i - 3].lower() == \"no\" and words_and_emoticons[i - 1].lower() in [\"or\", \"nor\"] ):\n",
    "                valence = self.lexicon[item_lowercase] * N_SCALAR\n",
    "\n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += C_INCR\n",
    "                else:\n",
    "                    valence -= C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                # dampen the scalar modifier of preceding words and emoticons\n",
    "                # (excluding the ones that immediately preceed the item) based\n",
    "                # on their distance from the current item.\n",
    "                if i > start_i and words_and_emoticons[i - (start_i + 1)].lower() not in self.lexicon:\n",
    "                    s = scalar_inc_dec(words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff)\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._negation_check(valence, words_and_emoticons, start_i, i)\n",
    "                    if start_i == 2:\n",
    "                        valence = self._special_idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if i > 1 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            if words_and_emoticons[i - 2].lower() != \"at\" and words_and_emoticons[i - 2].lower() != \"very\":\n",
    "                valence = valence * N_SCALAR\n",
    "        elif i > 0 and words_and_emoticons[i - 1].lower() not in self.lexicon \\\n",
    "                and words_and_emoticons[i - 1].lower() == \"least\":\n",
    "            valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _but_check(words_and_emoticons, sentiments):\n",
    "        # check for modification in sentiment due to contrastive conjunction 'but'\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if 'but' in words_and_emoticons_lower:\n",
    "            bi = words_and_emoticons_lower.index('but')\n",
    "            for sentiment in sentiments:\n",
    "                si = sentiments.index(sentiment)\n",
    "                if si < bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 0.5)\n",
    "                elif si > bi:\n",
    "                    sentiments.pop(si)\n",
    "                    sentiments.insert(si, sentiment * 1.5)\n",
    "        return sentiments\n",
    "\n",
    "    @staticmethod\n",
    "    def _special_idioms_check(valence, words_and_emoticons, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        onezero = \"{0} {1}\".format(words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoonezero = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 2],\n",
    "                                          words_and_emoticons_lower[i - 1], words_and_emoticons_lower[i])\n",
    "\n",
    "        twoone = \"{0} {1}\".format(words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwoone = \"{0} {1} {2}\".format(words_and_emoticons_lower[i - 3],\n",
    "                                           words_and_emoticons_lower[i - 2], words_and_emoticons_lower[i - 1])\n",
    "\n",
    "        threetwo = \"{0} {1}\".format(words_and_emoticons_lower[i - 3], words_and_emoticons_lower[i - 2])\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons_lower) - 1 > i:\n",
    "            zeroone = \"{0} {1}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1])\n",
    "            if zeroone in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroone]\n",
    "        if len(words_and_emoticons_lower) - 1 > i + 1:\n",
    "            zeroonetwo = \"{0} {1} {2}\".format(words_and_emoticons_lower[i], words_and_emoticons_lower[i + 1],\n",
    "                                              words_and_emoticons_lower[i + 2])\n",
    "            if zeroonetwo in SPECIAL_CASES:\n",
    "                valence = SPECIAL_CASES[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        n_grams = [threetwoone, threetwo, twoone]\n",
    "        for n_gram in n_grams:\n",
    "            if n_gram in BOOSTER_DICT:\n",
    "                valence = valence + BOOSTER_DICT[n_gram]\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _sentiment_laden_idioms_check(valence, senti_text_lower):\n",
    "        # Future Work\n",
    "        # check for sentiment laden idioms that don't contain a lexicon word\n",
    "        idioms_valences = []\n",
    "        for idiom in SENTIMENT_LADEN_IDIOMS:\n",
    "            if idiom in senti_text_lower:\n",
    "                print(idiom, senti_text_lower)\n",
    "                valence = SENTIMENT_LADEN_IDIOMS[idiom]\n",
    "                idioms_valences.append(valence)\n",
    "        if len(idioms_valences) > 0:\n",
    "            valence = sum(idioms_valences) / float(len(idioms_valences))\n",
    "        return valence\n",
    "\n",
    "    @staticmethod\n",
    "    def _negation_check(valence, words_and_emoticons, start_i, i):\n",
    "        words_and_emoticons_lower = [str(w).lower() for w in words_and_emoticons]\n",
    "        if start_i == 0:\n",
    "            if negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 1 word preceding lexicon word (w/o stopwords)\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons_lower[i - 2] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or\n",
    "                     words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 2] == \"without\" and \\\n",
    "                    words_and_emoticons_lower[i - 1] == \"doubt\":\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 2 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if words_and_emoticons_lower[i - 3] == \"never\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"so\" or words_and_emoticons_lower[i - 2] == \"this\") or \\\n",
    "                    (words_and_emoticons_lower[i - 1] == \"so\" or words_and_emoticons_lower[i - 1] == \"this\"):\n",
    "                valence = valence * 1.25\n",
    "            elif words_and_emoticons_lower[i - 3] == \"without\" and \\\n",
    "                    (words_and_emoticons_lower[i - 2] == \"doubt\" or words_and_emoticons_lower[i - 1] == \"doubt\"):\n",
    "                valence = valence\n",
    "            elif negated([words_and_emoticons_lower[i - (start_i + 1)]]):  # 3 words preceding the lexicon word position\n",
    "                valence = valence * N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_ep(text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _amplify_qm(text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    @staticmethod\n",
    "    def _sift_sentiment_scores(sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (float(sentiment_score) + 1)  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (float(sentiment_score) - 1)  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = \\\n",
    "            {\"neg\": round(neg, 3),\n",
    "             \"neu\": round(neu, 3),\n",
    "             \"pos\": round(pos, 3),\n",
    "             \"compound\": round(compound, 4)}\n",
    "\n",
    "        return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.192, 'neu': 0.808, 'pos': 0.0, 'compound': -0.2263}, {'neg': 0.084, 'neu': 0.745, 'pos': 0.171, 'compound': 0.4215}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.213, 'neu': 0.461, 'pos': 0.325, 'compound': 0.2349}, {'neg': 0.367, 'neu': 0.633, 'pos': 0.0, 'compound': -0.8516}, {'neg': 0.08, 'neu': 0.737, 'pos': 0.183, 'compound': 0.4939}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.359, 'neu': 0.641, 'pos': 0.0, 'compound': -0.4215}, {'neg': 0.239, 'neu': 0.761, 'pos': 0.0, 'compound': -0.296}, {'neg': 0.093, 'neu': 0.731, 'pos': 0.176, 'compound': 0.3612}, {'neg': 0.438, 'neu': 0.385, 'pos': 0.177, 'compound': -0.5267}, {'neg': 0.0, 'neu': 0.635, 'pos': 0.365, 'compound': 0.8979}, {'neg': 0.0, 'neu': 0.526, 'pos': 0.474, 'compound': 0.4019}, {'neg': 0.539, 'neu': 0.288, 'pos': 0.173, 'compound': -0.7479}, {'neg': 0.126, 'neu': 0.728, 'pos': 0.146, 'compound': 0.1027}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.723, 'pos': 0.277, 'compound': 0.3182}, {'neg': 0.014, 'neu': 0.776, 'pos': 0.21, 'compound': 0.9247}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.848, 'neu': 0.152, 'pos': 0.0, 'compound': -0.6808}, {'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'compound': 0.4215}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.103, 'neu': 0.84, 'pos': 0.057, 'compound': -0.0895}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.3818}, {'neg': 0.219, 'neu': 0.781, 'pos': 0.0, 'compound': -0.2023}, {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.4404}, {'neg': 0.108, 'neu': 0.857, 'pos': 0.036, 'compound': -0.9081}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.4939}, {'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'compound': -0.1531}, {'neg': 0.318, 'neu': 0.682, 'pos': 0.0, 'compound': -0.4215}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4019}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.676, 'neu': 0.324, 'pos': 0.0, 'compound': -0.9799}, {'neg': 0.569, 'neu': 0.172, 'pos': 0.259, 'compound': -0.4215}, {'neg': 0.039, 'neu': 0.831, 'pos': 0.131, 'compound': 0.8957}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.215, 'neu': 0.525, 'pos': 0.26, 'compound': 0.0207}, {'neg': 0.0, 'neu': 0.469, 'pos': 0.531, 'compound': 0.5267}, {'neg': 0.211, 'neu': 0.789, 'pos': 0.0, 'compound': -0.4588}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.097, 'neu': 0.818, 'pos': 0.085, 'compound': -0.1027}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.226, 'neu': 0.707, 'pos': 0.068, 'compound': -0.7003}, {'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.5423}, {'neg': 0.378, 'neu': 0.352, 'pos': 0.27, 'compound': -0.0588}, {'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.4526}, {'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.296}, {'neg': 0.305, 'neu': 0.519, 'pos': 0.175, 'compound': -0.4588}, {'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.6249}, {'neg': 0.094, 'neu': 0.906, 'pos': 0.0, 'compound': -0.3818}, {'neg': 0.114, 'neu': 0.667, 'pos': 0.219, 'compound': 0.4215}, {'neg': 0.481, 'neu': 0.127, 'pos': 0.392, 'compound': -0.1779}, {'neg': 0.291, 'neu': 0.573, 'pos': 0.136, 'compound': -0.3641}, {'neg': 0.064, 'neu': 0.833, 'pos': 0.103, 'compound': 0.3391}, {'neg': 0.209, 'neu': 0.791, 'pos': 0.0, 'compound': -0.4404}, {'neg': 0.0, 'neu': 0.919, 'pos': 0.081, 'compound': 0.0382}, {'neg': 0.032, 'neu': 0.483, 'pos': 0.485, 'compound': 0.9565}, {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'compound': 0.6597}, {'neg': 0.286, 'neu': 0.605, 'pos': 0.109, 'compound': -0.6597}, {'neg': 0.0, 'neu': 0.565, 'pos': 0.435, 'compound': 0.6597}, {'neg': 0.73, 'neu': 0.27, 'pos': 0.0, 'compound': -0.4019}, {'neg': 0.0, 'neu': 0.722, 'pos': 0.278, 'compound': 0.4019}, {'neg': 0.279, 'neu': 0.554, 'pos': 0.166, 'compound': -0.3008}, {'neg': 0.133, 'neu': 0.612, 'pos': 0.255, 'compound': 0.5541}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.698, 'pos': 0.302, 'compound': 0.0772}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.048, 'neu': 0.423, 'pos': 0.529, 'compound': 0.951}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.697, 'neu': 0.303, 'pos': 0.0, 'compound': -0.3182}, {'neg': 0.297, 'neu': 0.312, 'pos': 0.391, 'compound': 0.1531}, {'neg': 0.0, 'neu': 0.828, 'pos': 0.172, 'compound': 0.4019}, {'neg': 0.451, 'neu': 0.549, 'pos': 0.0, 'compound': -0.7506}, {'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.8934}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.71, 'pos': 0.29, 'compound': 0.6486}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.483, 'neu': 0.517, 'pos': 0.0, 'compound': -0.4215}, {'neg': 0.0, 'neu': 0.616, 'pos': 0.384, 'compound': 0.6808}, {'neg': 0.07, 'neu': 0.735, 'pos': 0.195, 'compound': 0.34}, {'neg': 0.297, 'neu': 0.27, 'pos': 0.432, 'compound': 0.25}, {'neg': 0.291, 'neu': 0.709, 'pos': 0.0, 'compound': -0.4019}, {'neg': 0.104, 'neu': 0.746, 'pos': 0.149, 'compound': 0.1531}, {'neg': 0.074, 'neu': 0.714, 'pos': 0.212, 'compound': 0.6369}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.6, 'pos': 0.4, 'compound': 0.25}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.355, 'neu': 0.645, 'pos': 0.0, 'compound': -0.296}, {'neg': 0.062, 'neu': 0.712, 'pos': 0.226, 'compound': 0.6705}, {'neg': 0.162, 'neu': 0.763, 'pos': 0.075, 'compound': -0.5878}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.5859}, {'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.5106}, {'neg': 0.0, 'neu': 0.303, 'pos': 0.697, 'compound': 0.7845}, {'neg': 0.127, 'neu': 0.816, 'pos': 0.057, 'compound': -0.3182}, {'neg': 0.007, 'neu': 0.919, 'pos': 0.074, 'compound': 0.9081}, {'neg': 0.027, 'neu': 0.784, 'pos': 0.188, 'compound': 0.802}, {'neg': 0.317, 'neu': 0.244, 'pos': 0.439, 'compound': 0.25}, {'neg': 0.0, 'neu': 0.119, 'pos': 0.881, 'compound': 0.8126}, {'neg': 0.172, 'neu': 0.586, 'pos': 0.243, 'compound': 0.1779}, {'neg': 0.388, 'neu': 0.331, 'pos': 0.281, 'compound': -0.5106}, {'neg': 0.141, 'neu': 0.634, 'pos': 0.225, 'compound': 0.296}, {'neg': 0.079, 'neu': 0.769, 'pos': 0.152, 'compound': 0.6705}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compound': -0.5574}, {'neg': 0.0, 'neu': 0.816, 'pos': 0.184, 'compound': 0.2023}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.169, 'neu': 0.739, 'pos': 0.092, 'compound': -0.4939}, {'neg': 0.292, 'neu': 0.622, 'pos': 0.086, 'compound': -0.7137}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.143, 'neu': 0.595, 'pos': 0.262, 'compound': 0.7632}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.194, 'neu': 0.583, 'pos': 0.223, 'compound': 0.0772}, {'neg': 0.535, 'neu': 0.174, 'pos': 0.29, 'compound': -0.3412}, {'neg': 0.348, 'neu': 0.435, 'pos': 0.217, 'compound': -0.128}, {'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.4215}, {'neg': 0.28, 'neu': 0.393, 'pos': 0.327, 'compound': 0.0248}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.451, 'neu': 0.278, 'pos': 0.271, 'compound': -0.5574}, {'neg': 0.244, 'neu': 0.311, 'pos': 0.444, 'compound': 0.6705}, {'neg': 0.163, 'neu': 0.337, 'pos': 0.5, 'compound': 0.7184}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.595, 'pos': 0.405, 'compound': 0.5267}, {'neg': 0.203, 'neu': 0.591, 'pos': 0.206, 'compound': 0.0258}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.113, 'neu': 0.62, 'pos': 0.267, 'compound': 0.802}, {'neg': 0.031, 'neu': 0.63, 'pos': 0.339, 'compound': 0.9136}, {'neg': 0.15, 'neu': 0.643, 'pos': 0.207, 'compound': 0.2035}, {'neg': 0.0, 'neu': 0.417, 'pos': 0.583, 'compound': 0.4215}, {'neg': 0.429, 'neu': 0.571, 'pos': 0.0, 'compound': -0.128}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.25}, {'neg': 0.243, 'neu': 0.644, 'pos': 0.114, 'compound': -0.9485}, {'neg': 0.173, 'neu': 0.827, 'pos': 0.0, 'compound': -0.3182}, {'neg': 0.0, 'neu': 0.648, 'pos': 0.352, 'compound': 0.765}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4019}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.474, 'neu': 0.526, 'pos': 0.0, 'compound': -0.5574}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.4019}, {'neg': 0.121, 'neu': 0.796, 'pos': 0.083, 'compound': -0.0284}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.222, 'neu': 0.256, 'pos': 0.521, 'compound': 0.5423}, {'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.2411}, {'neg': 0.299, 'neu': 0.677, 'pos': 0.024, 'compound': -0.9113}, {'neg': 0.318, 'neu': 0.353, 'pos': 0.33, 'compound': 0.0356}, {'neg': 0.352, 'neu': 0.336, 'pos': 0.312, 'compound': -0.1838}, {'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.4019}, {'neg': 0.167, 'neu': 0.731, 'pos': 0.102, 'compound': -0.3642}, {'neg': 0.265, 'neu': 0.573, 'pos': 0.162, 'compound': -0.1134}, {'neg': 0.459, 'neu': 0.541, 'pos': 0.0, 'compound': -0.6249}, {'neg': 0.02, 'neu': 0.915, 'pos': 0.066, 'compound': 0.4588}, {'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.6369}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.151, 'neu': 0.734, 'pos': 0.116, 'compound': 0.0258}, {'neg': 0.0, 'neu': 0.721, 'pos': 0.279, 'compound': 0.4767}, {'neg': 0.6, 'neu': 0.4, 'pos': 0.0, 'compound': -0.7184}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 0.545, 'pos': 0.455, 'compound': 0.6124}, {'neg': 0.094, 'neu': 0.416, 'pos': 0.489, 'compound': 0.8218}, {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.5859}, {'neg': 0.0, 'neu': 0.26, 'pos': 0.74, 'compound': 0.431}, {'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.4215}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.34, 'neu': 0.66, 'pos': 0.0, 'compound': -0.5574}, {'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.6705}, {'neg': 0.164, 'neu': 0.643, 'pos': 0.193, 'compound': 0.128}, {'neg': 0.0, 'neu': 0.662, 'pos': 0.338, 'compound': 0.5574}, {'neg': 0.217, 'neu': 0.698, 'pos': 0.085, 'compound': -0.4019}, {'neg': 0.0, 'neu': 0.794, 'pos': 0.206, 'compound': 0.9042}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.535, 'neu': 0.38, 'pos': 0.085, 'compound': -0.9481}, {'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.9231}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.042, 'neu': 0.832, 'pos': 0.126, 'compound': 0.5719}, {'neg': 0.0, 'neu': 0.519, 'pos': 0.481, 'compound': 0.5719}, {'neg': 0.0, 'neu': 0.811, 'pos': 0.189, 'compound': 0.5423}, {'neg': 0.377, 'neu': 0.557, 'pos': 0.066, 'compound': -0.9231}]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Apply the sentiment analysis function to the 'text' column\n",
    "sentiment_results = labelled['text'].apply(get_sentiment_scores)\n",
    "\n",
    "# Convert the results to a list of dictionaries\n",
    "sentiment_list = sentiment_results.tolist()\n",
    "\n",
    "# Display the sentiment list\n",
    "print(sentiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>link</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>moderation</th>\n",
       "      <th>original text</th>\n",
       "      <th>manual</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>972545</td>\n",
       "      <td>1057760</td>\n",
       "      <td>americans hold beer oh wait ive finished drinking</td>\n",
       "      <td>2022-01-24 05:25:27</td>\n",
       "      <td>MrFantasticallyNerdy</td>\n",
       "      <td>/r/SingaporeRaw/comments/sao76t/we_are_spendin...</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>htzg3uf</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>Americans: Hold my beer. Oh wait. I've already...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13347</td>\n",
       "      <td>14500</td>\n",
       "      <td>boss relies heavily pass holders start asking ...</td>\n",
       "      <td>2022-05-06 10:50:15</td>\n",
       "      <td>SimpleReadingSG90</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...</td>\n",
       "      <td>t3_ujhtgu</td>\n",
       "      <td>t1_i7j8gnf</td>\n",
       "      <td>i7jextx</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>And the boss who relies heavily on S Pass hold...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>461901</td>\n",
       "      <td>502383</td>\n",
       "      <td>mean youre paid much wipe youre tears money ea...</td>\n",
       "      <td>2022-05-08 05:02:44</td>\n",
       "      <td>OldFee3150</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujddhe/chan_chun_sing...</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>i7rioq7</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>I mean if youâ€™re being paid that much should j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1094356</td>\n",
       "      <td>1190080</td>\n",
       "      <td>lol idea hes singing singapore practice sang w...</td>\n",
       "      <td>2022-09-27 06:09:30</td>\n",
       "      <td>kopisiutaidaily</td>\n",
       "      <td>/r/SingaporeRaw/comments/xonblv/communist_taxi...</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>iq2n4xp</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>Lol he has no idea what heâ€™s singing. If Singa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1153986</td>\n",
       "      <td>1254859</td>\n",
       "      <td>would probably change sgkl railway line opens</td>\n",
       "      <td>2023-08-09 04:33:32</td>\n",
       "      <td>TaylorFritz</td>\n",
       "      <td>/r/SingaporeRaw/comments/15m2a52/why_do_singap...</td>\n",
       "      <td>t3_15m2a52</td>\n",
       "      <td>t1_jveg4qj</td>\n",
       "      <td>jveglrl</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>That would probably change once the SG-KL rail...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>847882</td>\n",
       "      <td>922205</td>\n",
       "      <td>yo thats night snack muffin ate last week haha...</td>\n",
       "      <td>2023-05-08 02:57:20</td>\n",
       "      <td>Rogueone65</td>\n",
       "      <td>/r/singapore/comments/13b8w1z/sq207_sin_to_mel...</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>jjaiifo</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Yo thatâ€™s the night snack muffin I ate last we...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1377594</td>\n",
       "      <td>1498060</td>\n",
       "      <td>hello thank posting singapore seems previously...</td>\n",
       "      <td>2023-08-21 16:01:39</td>\n",
       "      <td>ModeratelyUsefulBot</td>\n",
       "      <td>/r/singapore/comments/15x9vq1/public_hospitals...</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>jx5avr9</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Hello and thank you for posting to singapore! ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1498756</td>\n",
       "      <td>1629859</td>\n",
       "      <td>guys celebrate vesak day too</td>\n",
       "      <td>2023-05-26 22:18:28</td>\n",
       "      <td>brownriver12</td>\n",
       "      <td>/r/singapore/comments/13spsi3/rsingapore_rando...</td>\n",
       "      <td>t3_13spsi3</td>\n",
       "      <td>t1_jlr58id</td>\n",
       "      <td>jlr5ecb</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>You guys celebrate vesak day too?ðŸ¤£</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1312718</td>\n",
       "      <td>1427653</td>\n",
       "      <td>stand upon shoulders giants walked may run ins...</td>\n",
       "      <td>2022-08-24 04:03:51</td>\n",
       "      <td>fish312</td>\n",
       "      <td>/r/singapore/comments/ww82t0/from_aug_29_masks...</td>\n",
       "      <td>t3_ww82t0</td>\n",
       "      <td>t1_iljpegw</td>\n",
       "      <td>iljvxm7</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'controversiality': 0, 'collapsed_reason_code...</td>\n",
       "      <td>We stand upon the shoulders of giants. She wal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>655848</td>\n",
       "      <td>713362</td>\n",
       "      <td>oh crap skimmed need tattoo removal services d...</td>\n",
       "      <td>2023-04-29 03:14:22</td>\n",
       "      <td>Turbografx220</td>\n",
       "      <td>/r/singapore/comments/132h8jg/tattoo_removals_...</td>\n",
       "      <td>t3_132h8jg</td>\n",
       "      <td>t1_ji53apm</td>\n",
       "      <td>ji53onk</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>{'banned_at_utc': None, 'mod_reason_by': None,...</td>\n",
       "      <td>Oh crap I skimmed it.\\n\\nWe need more of tatto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.9231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0          972545     1057760   \n",
       "1           13347       14500   \n",
       "2          461901      502383   \n",
       "3         1094356     1190080   \n",
       "4         1153986     1254859   \n",
       "..            ...         ...   \n",
       "195        847882      922205   \n",
       "196       1377594     1498060   \n",
       "197       1498756     1629859   \n",
       "198       1312718     1427653   \n",
       "199        655848      713362   \n",
       "\n",
       "                                                  text            timestamp  \\\n",
       "0    americans hold beer oh wait ive finished drinking  2022-01-24 05:25:27   \n",
       "1    boss relies heavily pass holders start asking ...  2022-05-06 10:50:15   \n",
       "2    mean youre paid much wipe youre tears money ea...  2022-05-08 05:02:44   \n",
       "3    lol idea hes singing singapore practice sang w...  2022-09-27 06:09:30   \n",
       "4        would probably change sgkl railway line opens  2023-08-09 04:33:32   \n",
       "..                                                 ...                  ...   \n",
       "195  yo thats night snack muffin ate last week haha...  2023-05-08 02:57:20   \n",
       "196  hello thank posting singapore seems previously...  2023-08-21 16:01:39   \n",
       "197                       guys celebrate vesak day too  2023-05-26 22:18:28   \n",
       "198  stand upon shoulders giants walked may run ins...  2022-08-24 04:03:51   \n",
       "199  oh crap skimmed need tattoo removal services d...  2023-04-29 03:14:22   \n",
       "\n",
       "                 username                                               link  \\\n",
       "0    MrFantasticallyNerdy  /r/SingaporeRaw/comments/sao76t/we_are_spendin...   \n",
       "1       SimpleReadingSG90  /r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...   \n",
       "2              OldFee3150  /r/SingaporeRaw/comments/ujddhe/chan_chun_sing...   \n",
       "3         kopisiutaidaily  /r/SingaporeRaw/comments/xonblv/communist_taxi...   \n",
       "4             TaylorFritz  /r/SingaporeRaw/comments/15m2a52/why_do_singap...   \n",
       "..                    ...                                                ...   \n",
       "195            Rogueone65  /r/singapore/comments/13b8w1z/sq207_sin_to_mel...   \n",
       "196   ModeratelyUsefulBot  /r/singapore/comments/15x9vq1/public_hospitals...   \n",
       "197          brownriver12  /r/singapore/comments/13spsi3/rsingapore_rando...   \n",
       "198               fish312  /r/singapore/comments/ww82t0/from_aug_29_masks...   \n",
       "199         Turbografx220  /r/singapore/comments/132h8jg/tattoo_removals_...   \n",
       "\n",
       "        link_id   parent_id       id subreddit_id  \\\n",
       "0     t3_sao76t   t3_sao76t  htzg3uf     t5_xnx04   \n",
       "1     t3_ujhtgu  t1_i7j8gnf  i7jextx     t5_xnx04   \n",
       "2     t3_ujddhe   t3_ujddhe  i7rioq7     t5_xnx04   \n",
       "3     t3_xonblv   t3_xonblv  iq2n4xp     t5_xnx04   \n",
       "4    t3_15m2a52  t1_jveg4qj  jveglrl     t5_xnx04   \n",
       "..          ...         ...      ...          ...   \n",
       "195  t3_13b8w1z  t3_13b8w1z  jjaiifo     t5_2qh8c   \n",
       "196  t3_15x9vq1  t3_15x9vq1  jx5avr9     t5_2qh8c   \n",
       "197  t3_13spsi3  t1_jlr58id  jlr5ecb     t5_2qh8c   \n",
       "198   t3_ww82t0  t1_iljpegw  iljvxm7     t5_2qh8c   \n",
       "199  t3_132h8jg  t1_ji53apm  ji53onk     t5_2qh8c   \n",
       "\n",
       "                                            moderation  \\\n",
       "0    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "1    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "2    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "3    {'controversiality': 0, 'collapsed_reason_code...   \n",
       "4    {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "..                                                 ...   \n",
       "195  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "196  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "197  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "198  {'controversiality': 0, 'collapsed_reason_code...   \n",
       "199  {'banned_at_utc': None, 'mod_reason_by': None,...   \n",
       "\n",
       "                                         original text  manual    neg    neu  \\\n",
       "0    Americans: Hold my beer. Oh wait. I've already...       0  0.000  1.000   \n",
       "1    And the boss who relies heavily on S Pass hold...       0  0.000  1.000   \n",
       "2    I mean if youâ€™re being paid that much should j...       0  0.192  0.808   \n",
       "3    Lol he has no idea what heâ€™s singing. If Singa...       0  0.084  0.745   \n",
       "4    That would probably change once the SG-KL rail...       0  0.000  1.000   \n",
       "..                                                 ...     ...    ...    ...   \n",
       "195  Yo thatâ€™s the night snack muffin I ate last we...       0  0.000  1.000   \n",
       "196  Hello and thank you for posting to singapore! ...       0  0.042  0.832   \n",
       "197                 You guys celebrate vesak day too?ðŸ¤£       0  0.000  0.519   \n",
       "198  We stand upon the shoulders of giants. She wal...       0  0.000  0.811   \n",
       "199  Oh crap I skimmed it.\\n\\nWe need more of tatto...       0  0.377  0.557   \n",
       "\n",
       "       pos  compound  \n",
       "0    0.000    0.0000  \n",
       "1    0.000    0.0000  \n",
       "2    0.000   -0.2263  \n",
       "3    0.171    0.4215  \n",
       "4    0.000    0.0000  \n",
       "..     ...       ...  \n",
       "195  0.000    0.0000  \n",
       "196  0.126    0.5719  \n",
       "197  0.481    0.5719  \n",
       "198  0.189    0.5423  \n",
       "199  0.066   -0.9231  \n",
       "\n",
       "[200 rows x 17 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we append it to our original dataset\n",
    "results_df = pd.DataFrame(sentiment_list)\n",
    "\n",
    "labelled.reset_index(drop=True, inplace=True)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "vadar_df = pd.concat([labelled, results_df], axis = 1)\n",
    "vadar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\"ahbeng\", \"ahlian\", \"baka\", \"bloody hell\", \"bloody idiot\", \"bodoh\", \"bo liao\", \"buay pai seh\", \"buay tahan\", \"cb\", \"cb kia\", \"cb knn\", \"cb\", \"cb lao jia\", \"cb lao knn\", \"cb lor\", \"cb sia\", \"cb sia kia\", \"ccb\", \"chbye kia\", \"chao chbye\", \"chao chee bye\", \"chow chibai\", \"chow kar\", \"chow tu lan\", \"cibai\", \"dumb ass\", \"dumb\", \"fuck\", \"fuck you\", \"fking\", \"fucker\", \"fucker sia\", \"gila babi\", \"gundu\", \"hao lian kia\", \"hopeless\", \"idiot\", \"idiot\", \"ji bai\", \"jiat lat\", \"jialat kia\", \"jibai\", \"joker\", \"kan\", \"kan ni na\", \"kena sai\", \"kia si lang\", \"knn\", \"knn cb kia\", \"knnccb\", \"knnbccb\", \"kns\", \"kns cb\", \"lampa\", \"lan pa\", \"lanjiao\", \"lanjiao kia\", \"lj\", \"loser\", \"nabei\", \"no use kia\", \"noob\", \"pok gai\", \"pui\", \"sabo kia\", \"sibei jialat\", \"sibei sian\", \"si gina\", \"siol\", \"slut\", \"siao lang\", \"stupid\", \"suck\", \"sua gu\", \"tmd\", \"tiok knn\", \"tiok tiam\", \"useless\", \"what knn\", \"what the fuck\", \"wtf\", \"wu liao kia\", \"you die ah\", \"you die\", \"pengsan\", \"pong teng\", \"sabo\", \"paktor\", 'Gila', 'Mampus', 'makan', 'kopi', 'teh', 'boleh', 'habis', 'lagi', 'tak', 'tak boleh', 'tak ada', 'takde', 'mesti', 'sudah', 'alamak', 'keluar', 'baik', 'siapa', 'apa', 'mana', 'banyak', 'kena', 'otah', 'otak', 'kiasu', 'kiasi', 'atas', 'bawah', 'liddat', 'suka', 'masak', 'lagi', 'boh', 'kanasai', 'jialat', 'gao', 'liak', 'pang', 'chey', 'guai', 'chio', 'gahmen', 'shiok', 'tok', 'zai', 'zhao', 'lahhh', 'wahpiang', 'heng', 'suay', 'boh liao', 'kangkang', 'chiobu', 'chapalang', 'zhu', 'gong', 'suay', 'kan', 'jian', 'buey', 'tio', 'steady bom pee pee', 'baey', 'macam', 'walaueh', 'wahlao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>link</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>...</th>\n",
       "      <th>manual</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>toxic_0.4</th>\n",
       "      <th>toxic_0.5</th>\n",
       "      <th>toxic_0.6</th>\n",
       "      <th>toxic_0.7</th>\n",
       "      <th>toxic_0.8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>972545</td>\n",
       "      <td>1057760</td>\n",
       "      <td>americans hold beer oh wait ive finished drinking</td>\n",
       "      <td>2022-01-24 05:25:27</td>\n",
       "      <td>MrFantasticallyNerdy</td>\n",
       "      <td>/r/SingaporeRaw/comments/sao76t/we_are_spendin...</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>t3_sao76t</td>\n",
       "      <td>htzg3uf</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13347</td>\n",
       "      <td>14500</td>\n",
       "      <td>boss relies heavily pass holders start asking ...</td>\n",
       "      <td>2022-05-06 10:50:15</td>\n",
       "      <td>SimpleReadingSG90</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...</td>\n",
       "      <td>t3_ujhtgu</td>\n",
       "      <td>t1_i7j8gnf</td>\n",
       "      <td>i7jextx</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>461901</td>\n",
       "      <td>502383</td>\n",
       "      <td>mean youre paid much wipe youre tears money ea...</td>\n",
       "      <td>2022-05-08 05:02:44</td>\n",
       "      <td>OldFee3150</td>\n",
       "      <td>/r/SingaporeRaw/comments/ujddhe/chan_chun_sing...</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>t3_ujddhe</td>\n",
       "      <td>i7rioq7</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1094356</td>\n",
       "      <td>1190080</td>\n",
       "      <td>lol idea hes singing singapore practice sang w...</td>\n",
       "      <td>2022-09-27 06:09:30</td>\n",
       "      <td>kopisiutaidaily</td>\n",
       "      <td>/r/SingaporeRaw/comments/xonblv/communist_taxi...</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>t3_xonblv</td>\n",
       "      <td>iq2n4xp</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1153986</td>\n",
       "      <td>1254859</td>\n",
       "      <td>would probably change sgkl railway line opens</td>\n",
       "      <td>2023-08-09 04:33:32</td>\n",
       "      <td>TaylorFritz</td>\n",
       "      <td>/r/SingaporeRaw/comments/15m2a52/why_do_singap...</td>\n",
       "      <td>t3_15m2a52</td>\n",
       "      <td>t1_jveg4qj</td>\n",
       "      <td>jveglrl</td>\n",
       "      <td>t5_xnx04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>847882</td>\n",
       "      <td>922205</td>\n",
       "      <td>yo thats night snack muffin ate last week haha...</td>\n",
       "      <td>2023-05-08 02:57:20</td>\n",
       "      <td>Rogueone65</td>\n",
       "      <td>/r/singapore/comments/13b8w1z/sq207_sin_to_mel...</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>t3_13b8w1z</td>\n",
       "      <td>jjaiifo</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1377594</td>\n",
       "      <td>1498060</td>\n",
       "      <td>hello thank posting singapore seems previously...</td>\n",
       "      <td>2023-08-21 16:01:39</td>\n",
       "      <td>ModeratelyUsefulBot</td>\n",
       "      <td>/r/singapore/comments/15x9vq1/public_hospitals...</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>t3_15x9vq1</td>\n",
       "      <td>jx5avr9</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1498756</td>\n",
       "      <td>1629859</td>\n",
       "      <td>guys celebrate vesak day too</td>\n",
       "      <td>2023-05-26 22:18:28</td>\n",
       "      <td>brownriver12</td>\n",
       "      <td>/r/singapore/comments/13spsi3/rsingapore_rando...</td>\n",
       "      <td>t3_13spsi3</td>\n",
       "      <td>t1_jlr58id</td>\n",
       "      <td>jlr5ecb</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1312718</td>\n",
       "      <td>1427653</td>\n",
       "      <td>stand upon shoulders giants walked may run ins...</td>\n",
       "      <td>2022-08-24 04:03:51</td>\n",
       "      <td>fish312</td>\n",
       "      <td>/r/singapore/comments/ww82t0/from_aug_29_masks...</td>\n",
       "      <td>t3_ww82t0</td>\n",
       "      <td>t1_iljpegw</td>\n",
       "      <td>iljvxm7</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>655848</td>\n",
       "      <td>713362</td>\n",
       "      <td>oh crap skimmed need tattoo removal services d...</td>\n",
       "      <td>2023-04-29 03:14:22</td>\n",
       "      <td>Turbografx220</td>\n",
       "      <td>/r/singapore/comments/132h8jg/tattoo_removals_...</td>\n",
       "      <td>t3_132h8jg</td>\n",
       "      <td>t1_ji53apm</td>\n",
       "      <td>ji53onk</td>\n",
       "      <td>t5_2qh8c</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.9231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0  \\\n",
       "0          972545     1057760   \n",
       "1           13347       14500   \n",
       "2          461901      502383   \n",
       "3         1094356     1190080   \n",
       "4         1153986     1254859   \n",
       "..            ...         ...   \n",
       "195        847882      922205   \n",
       "196       1377594     1498060   \n",
       "197       1498756     1629859   \n",
       "198       1312718     1427653   \n",
       "199        655848      713362   \n",
       "\n",
       "                                                  text            timestamp  \\\n",
       "0    americans hold beer oh wait ive finished drinking  2022-01-24 05:25:27   \n",
       "1    boss relies heavily pass holders start asking ...  2022-05-06 10:50:15   \n",
       "2    mean youre paid much wipe youre tears money ea...  2022-05-08 05:02:44   \n",
       "3    lol idea hes singing singapore practice sang w...  2022-09-27 06:09:30   \n",
       "4        would probably change sgkl railway line opens  2023-08-09 04:33:32   \n",
       "..                                                 ...                  ...   \n",
       "195  yo thats night snack muffin ate last week haha...  2023-05-08 02:57:20   \n",
       "196  hello thank posting singapore seems previously...  2023-08-21 16:01:39   \n",
       "197                       guys celebrate vesak day too  2023-05-26 22:18:28   \n",
       "198  stand upon shoulders giants walked may run ins...  2022-08-24 04:03:51   \n",
       "199  oh crap skimmed need tattoo removal services d...  2023-04-29 03:14:22   \n",
       "\n",
       "                 username                                               link  \\\n",
       "0    MrFantasticallyNerdy  /r/SingaporeRaw/comments/sao76t/we_are_spendin...   \n",
       "1       SimpleReadingSG90  /r/SingaporeRaw/comments/ujhtgu/rants_of_a_19y...   \n",
       "2              OldFee3150  /r/SingaporeRaw/comments/ujddhe/chan_chun_sing...   \n",
       "3         kopisiutaidaily  /r/SingaporeRaw/comments/xonblv/communist_taxi...   \n",
       "4             TaylorFritz  /r/SingaporeRaw/comments/15m2a52/why_do_singap...   \n",
       "..                    ...                                                ...   \n",
       "195            Rogueone65  /r/singapore/comments/13b8w1z/sq207_sin_to_mel...   \n",
       "196   ModeratelyUsefulBot  /r/singapore/comments/15x9vq1/public_hospitals...   \n",
       "197          brownriver12  /r/singapore/comments/13spsi3/rsingapore_rando...   \n",
       "198               fish312  /r/singapore/comments/ww82t0/from_aug_29_masks...   \n",
       "199         Turbografx220  /r/singapore/comments/132h8jg/tattoo_removals_...   \n",
       "\n",
       "        link_id   parent_id       id subreddit_id  ... manual    neg    neu  \\\n",
       "0     t3_sao76t   t3_sao76t  htzg3uf     t5_xnx04  ...      0  0.000  1.000   \n",
       "1     t3_ujhtgu  t1_i7j8gnf  i7jextx     t5_xnx04  ...      0  0.000  1.000   \n",
       "2     t3_ujddhe   t3_ujddhe  i7rioq7     t5_xnx04  ...      0  0.192  0.808   \n",
       "3     t3_xonblv   t3_xonblv  iq2n4xp     t5_xnx04  ...      0  0.084  0.745   \n",
       "4    t3_15m2a52  t1_jveg4qj  jveglrl     t5_xnx04  ...      0  0.000  1.000   \n",
       "..          ...         ...      ...          ...  ...    ...    ...    ...   \n",
       "195  t3_13b8w1z  t3_13b8w1z  jjaiifo     t5_2qh8c  ...      0  0.000  1.000   \n",
       "196  t3_15x9vq1  t3_15x9vq1  jx5avr9     t5_2qh8c  ...      0  0.042  0.832   \n",
       "197  t3_13spsi3  t1_jlr58id  jlr5ecb     t5_2qh8c  ...      0  0.000  0.519   \n",
       "198   t3_ww82t0  t1_iljpegw  iljvxm7     t5_2qh8c  ...      0  0.000  0.811   \n",
       "199  t3_132h8jg  t1_ji53apm  ji53onk     t5_2qh8c  ...      0  0.377  0.557   \n",
       "\n",
       "       pos  compound  toxic_0.4  toxic_0.5  toxic_0.6  toxic_0.7  toxic_0.8  \n",
       "0    0.000    0.0000          0          0          0          0          0  \n",
       "1    0.000    0.0000          0          0          0          0          0  \n",
       "2    0.000   -0.2263          0          0          0          0          0  \n",
       "3    0.171    0.4215          0          0          0          0          0  \n",
       "4    0.000    0.0000          0          0          0          0          0  \n",
       "..     ...       ...        ...        ...        ...        ...        ...  \n",
       "195  0.000    0.0000          0          0          0          0          0  \n",
       "196  0.126    0.5719          1          1          1          1          1  \n",
       "197  0.481    0.5719          0          0          0          0          0  \n",
       "198  0.189    0.5423          0          0          0          0          0  \n",
       "199  0.066   -0.9231          1          1          1          1          1  \n",
       "\n",
       "[200 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vadar_df['toxic_0.4'] = 0\n",
    "vadar_df['toxic_0.5'] = 0\n",
    "vadar_df['toxic_0.6'] = 0\n",
    "vadar_df['toxic_0.7'] = 0\n",
    "vadar_df['toxic_0.8'] = 0\n",
    "\n",
    "vadar_df.loc[vadar_df['compound'] <= -0.4, 'toxic_0.4'] = 1\n",
    "vadar_df.loc[vadar_df['compound'] <= -0.5, 'toxic_0.5'] = 1\n",
    "vadar_df.loc[vadar_df['compound'] <= -0.6, 'toxic_0.6'] = 1\n",
    "vadar_df.loc[vadar_df['compound'] <= -0.7, 'toxic_0.7'] = 1\n",
    "vadar_df.loc[vadar_df['compound'] <= -0.8, 'toxic_0.8'] = 1\n",
    "\n",
    "vadar_df['toxic_0.4'] = np.where(vadar_df['text'].str.contains('|'.join(lst), case=False, na=False), 1, vadar_df['toxic_0.4'])\n",
    "vadar_df['toxic_0.5'] = np.where(vadar_df['text'].str.contains('|'.join(lst), case=False, na=False), 1, vadar_df['toxic_0.5'])\n",
    "vadar_df['toxic_0.6'] = np.where(vadar_df['text'].str.contains('|'.join(lst), case=False, na=False), 1, vadar_df['toxic_0.6'])\n",
    "vadar_df['toxic_0.7'] = np.where(vadar_df['text'].str.contains('|'.join(lst), case=False, na=False), 1, vadar_df['toxic_0.7'])\n",
    "vadar_df['toxic_0.8'] = np.where(vadar_df['text'].str.contains('|'.join(lst), case=False, na=False), 1, vadar_df['toxic_0.8'])\n",
    "vadar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   threshold  TP  FP  FN   TN  precision  recall\n",
      "0        0.4  20  44  20  116   0.312500   0.500\n",
      "1        0.5  19  36  21  124   0.345455   0.475\n",
      "2        0.6  15  35  25  125   0.300000   0.375\n",
      "3        0.7  14  33  26  127   0.297872   0.350\n",
      "4        0.8  12  32  28  128   0.272727   0.300\n"
     ]
    }
   ],
   "source": [
    "# Define ticker values\n",
    "abs_threshold = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the ticker values\n",
    "for t in abs_threshold:\n",
    "    \n",
    "    # Get the actual and predicted labels\n",
    "    y_true = vadar_df['manual']\n",
    "    y_pred = vadar_df[f'toxic_{t}']\n",
    "\n",
    "    # Calculate confusion matrix for 'non-toxic' vs 'toxic'\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    # Store the results in the list\n",
    "    results.append({\n",
    "        'threshold': t,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TN': tn,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
