{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Singlish toxic words dictionary\n",
    "singlish_toxic_dict = ['ahbeng', 'ahlian', 'baka', 'bloody hell', 'bloody idiot', 'bodoh', 'bo liao',\n",
    "                       'buay pai seh', 'buay tahan', 'cb', 'cb kia', 'cb knn', 'cb', 'cb lao jia', 'cb lao knn',\n",
    "                       'cb lor', 'cb sia', 'cb sia kia', 'ccb', 'chbye kia', 'chao chbye', 'chao chee bye',\n",
    "                       'chow cibai', 'chow kar', 'chow tu lan', 'cibai', 'dumb ass', 'dumb', 'fuck', 'fuck you', 'fking', 'fucker',\n",
    "                       'fucker sia', 'gila babi', 'gundu', 'hao lian kia', 'hopeless', 'idiot', 'idiot', 'ji bai',\n",
    "                       'jiat lat', 'jialat kia', 'jibai', 'joker', 'kan', 'kan ni na', 'kena sai', 'kia si lang', 'knn',\n",
    "                       'knn cb kia', 'knnccb', 'knnbccb', 'kns', 'kns cb', 'lampa', 'lan pa', 'lanjiao', 'lanjiao kia', 'lj', 'loser', 'nabei',\n",
    "                       'no use kia', 'noob', 'pok gai', 'pui', 'sabo kia', 'sibei jialat', 'sibei sian', 'si gina', 'siol', 'slut',\n",
    "                       'siao lang', 'stupid', 'suck', 'sua gu', 'tmd', 'tiok knn', 'tiok tiam', 'useless', 'what knn', 'what the fuck', 'wtf',\n",
    "                       'wu liao kia', 'you die ah', 'you die']"
   ],
   "id": "4f551479336b7862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# List of models and which ones to include in output\n",
    "models_config = [\n",
    "    ('unitary/toxic-bert', True),  # Include this one\n",
    "    ('unitary/unbiased-toxic-roberta', True),  # Include this one\n",
    "    ('pykeio/lite-toxic-comment-classification', True),  # Include this one\n",
    "    ('martin-ha/toxic-comment-model', False),\n",
    "    ('JungleLee/bert-toxic-comment-classification', False),\n",
    "    ('ZiruiXiong/bert-base-finetuned-toxic-comment-classification', False),\n",
    "    ('longluu/distilbert-toxic-comment-classifier', False),\n",
    "    ('prabhaskenche/toxic-comment-classification-using-RoBERTa', False),\n",
    "]\n",
    "\n",
    "# Get all model names for processing\n",
    "model_names = [model[0] for model in models_config]\n",
    "\n",
    "# Get models to include in output\n",
    "models_to_output = [model[0] for model in models_config if model[1]]"
   ],
   "id": "292cd4e6416eefc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def convert_manual_label(label):\n",
    "    return \"toxic\" if label == 1 else \"non-toxic\"\n",
    "\n",
    "def flag_singlish_toxic(text, toxic_dict):\n",
    "    if isinstance(text, str):\n",
    "        for word in toxic_dict:\n",
    "            if word in text.lower():\n",
    "                return \"toxic\"\n",
    "    return None\n",
    "\n",
    "def classify_toxicity_by_dynamic_chunks(text, tokenizer, classifier, max_length=512):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        total_tokens = len(input_ids)\n",
    "\n",
    "        if total_tokens <= max_length:\n",
    "            num_chunks = 1\n",
    "            chunk_size = total_tokens\n",
    "        else:\n",
    "            num_chunks = math.ceil(total_tokens / max_length)\n",
    "            chunk_size = math.ceil(total_tokens / num_chunks)\n",
    "\n",
    "        toxicity_scores = []\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start = i * chunk_size\n",
    "            end = min((i + 1) * chunk_size, total_tokens)\n",
    "            chunk = tokenizer.decode(input_ids[start:end], skip_special_tokens=True)\n",
    "            result = classifier(chunk)\n",
    "            toxicity_scores.append(result[0]['score'])\n",
    "\n",
    "        return sum(toxicity_scores) / len(toxicity_scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error processing text chunk: {str(e)}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_metrics(actual_labels, scores, dict_predictions, threshold):\n",
    "    predicted_labels = []\n",
    "    for i, score in enumerate(scores):\n",
    "        if dict_predictions[i] == \"toxic\":\n",
    "            predicted_labels.append(\"toxic\")\n",
    "        else:\n",
    "            predicted_labels.append(\"toxic\" if score > threshold else \"non-toxic\")\n",
    "\n",
    "    tp = sum((a == 'toxic' and p == 'toxic') for a, p in zip(actual_labels, predicted_labels))\n",
    "    fp = sum((a == 'non-toxic' and p == 'toxic') for a, p in zip(actual_labels, predicted_labels))\n",
    "    fn = sum((a == 'toxic' and p == 'non-toxic') for a, p in zip(actual_labels, predicted_labels))\n",
    "    tn = sum((a == 'non-toxic' and p == 'non-toxic') for a, p in zip(actual_labels, predicted_labels))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tn': tn,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ],
   "id": "373e9569bc51a9d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Main execution\n",
    "# Get the current working directory (LLM folder)\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Go up one level to parent directory and then into data folder\n",
    "data_dir = os.path.join(os.path.dirname(current_dir), 'data')\n",
    "file_path_sample = os.path.join(data_dir, 'manual_label_sample.xlsx')\n",
    "excel_output_path = os.path.join(data_dir, 'model_metrics.xlsx')\n",
    "\n",
    "df_sample = pd.read_excel(file_path_sample)\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "# Convert manual labels to toxic/non-toxic\n",
    "actual_labels = df_sample['manual'].apply(convert_manual_label).tolist()\n",
    "\n",
    "# First, apply dictionary-based flagging\n",
    "dict_predictions = [flag_singlish_toxic(text, singlish_toxic_dict) for text in df_sample['text']]\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Process all models\n",
    "for model_name in model_names:\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Calculate toxicity scores\n",
    "    scores = []\n",
    "    for i, text in enumerate(df_sample['text']):\n",
    "        if dict_predictions[i] == \"toxic\":\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            score = classify_toxicity_by_dynamic_chunks(text, tokenizer, classifier)\n",
    "            scores.append(score)\n",
    "\n",
    "    # Calculate and sort metrics for each threshold\n",
    "    metrics_list = []\n",
    "    for threshold in thresholds:\n",
    "        metrics = calculate_metrics(actual_labels, scores, dict_predictions, threshold)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "    # Sort by recall score\n",
    "    metrics_list.sort(key=lambda x: x['recall'], reverse=True)\n",
    "    all_results[model_name] = metrics_list"
   ],
   "id": "711bca5584fdb599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Excel output only for selected models\n",
    "with pd.ExcelWriter(excel_output_path, engine='xlsxwriter') as writer:\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet('Metrics')\n",
    "\n",
    "    # Add formats\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'font_size': 12,\n",
    "        'bg_color': '#4F81BD',\n",
    "        'font_color': 'white',\n",
    "        'border': 1\n",
    "    })\n",
    "\n",
    "    subheader_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'font_size': 11,\n",
    "        'bg_color': '#D3D3D3',\n",
    "        'border': 1\n",
    "    })\n",
    "\n",
    "    cell_format = workbook.add_format({\n",
    "        'border': 1\n",
    "    })\n",
    "\n",
    "    # Initialize row counter\n",
    "    current_row = 0\n",
    "\n",
    "    # Write only selected models to Excel\n",
    "    for model_name in models_to_output:\n",
    "        # Write model name header\n",
    "        worksheet.merge_range(current_row, 0, current_row, 6, f\"Model: {model_name}\", header_format)\n",
    "        current_row += 1\n",
    "\n",
    "        # Write column headers\n",
    "        headers = ['Threshold', 'TP', 'FP', 'FN', 'TN', 'Precision', 'Recall']\n",
    "        for col, header in enumerate(headers):\n",
    "            worksheet.write(current_row, col, header, subheader_format)\n",
    "        current_row += 1\n",
    "\n",
    "        # Write metrics data\n",
    "        for metrics in all_results[model_name]:\n",
    "            worksheet.write(current_row, 0, metrics['threshold'], cell_format)\n",
    "            worksheet.write(current_row, 1, metrics['tp'], cell_format)\n",
    "            worksheet.write(current_row, 2, metrics['fp'], cell_format)\n",
    "            worksheet.write(current_row, 3, metrics['fn'], cell_format)\n",
    "            worksheet.write(current_row, 4, metrics['tn'], cell_format)\n",
    "            worksheet.write(current_row, 5, metrics['precision'], cell_format)\n",
    "            worksheet.write(current_row, 6, metrics['recall'], cell_format)\n",
    "            current_row += 1\n",
    "\n",
    "        # Add a blank row between models\n",
    "        current_row += 1\n",
    "\n",
    "    # Adjust column widths\n",
    "    worksheet.set_column(0, 0, 12)  # Threshold\n",
    "    worksheet.set_column(1, 4, 8)  # TP, FP, FN, TN\n",
    "    worksheet.set_column(5, 6, 12)  # Precision, Recall\n",
    "\n",
    "print(f\"Results have been saved to {excel_output_path}\")"
   ],
   "id": "79cab332bdeda68e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
