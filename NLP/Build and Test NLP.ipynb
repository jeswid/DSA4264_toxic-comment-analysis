{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Data Cleaning"],"metadata":{"id":"gIO5yWCWoDjM"}},{"cell_type":"code","source":["# Load cleaned data from Excel\n","df = pd.read_csv('cleaned_data_2223.csv')"],"metadata":{"id":"jdJ_tWYBnBrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Filter out comments <= 5 words\n","df = df[df['text'].str.split().str.len() > 5]"],"metadata":{"id":"l7eoAEACnDSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##NLP Using Spacy for classifying negative comments"],"metadata":{"id":"o4mHFK01Oxau"}},{"cell_type":"code","source":["#topic modelling attempt for year 2022-2023 using LDA model\n","# Import necessary libraries\n","import re\n","import pandas as pd\n","import spacy\n","from spacytextblob.spacytextblob import SpacyTextBlob\n","import sklearn\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from tqdm import tqdm\n","import gensim\n","from gensim.models import CoherenceModel\n","from gensim.corpora import Dictionary"],"metadata":{"id":"Bi4LkEYwO9NS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import spacy\n","from spacytextblob.spacytextblob import SpacyTextBlob"],"metadata":{"id":"XwFuO2mQm9p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialise the NLP pipeline and add the spacytextblob step to the pipeline\n","nlp = spacy.load('en_core_web_sm')\n","nlp.add_pipe('spacytextblob')\n","\n","# Add your texts (assuming grouped_df['Speech'] contains the text data)\n","texts = df['text']\n","\n","# This will take about 20-30 seconds to run\n","sentiment_results = []\n","for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"ner\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n","    sentiment_results.append({\n","        'Polarity': doc._.blob.polarity,\n","        'Subjectivity': doc._.blob.subjectivity,\n","    })\n","\n","# Convert sentiment results into a DataFrame\n","sentiment_results_df = pd.DataFrame(sentiment_results)\n","\n","# Append sentiment results to the original dataset\n","grouped_df = pd.concat([df, sentiment_results_df], axis=1)"],"metadata":{"id":"1cEq5CGznFmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#try seeing if hve parent comments - but we realised that looking only at parent comments to do topic modelling was not accurate due to too few comments being analysed\n","parents_raw = grouped_df[grouped_df['link_id'] == grouped_df['parent_id'].str.replace(r\"^t\\d+_\", \"\", regex=True)]"],"metadata":{"id":"0hmSBonunL0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter out comments with negative sentiment (polarity below -0.2)\n","negative_comments_df = grouped_df[grouped_df['Polarity'] < -0.2]"],"metadata":{"id":"7l-1OHGYnUQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display or process the filtered DataFrame\n","negative_comments_df.head()"],"metadata":{"id":"e-sUGplTnV_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##NLP pipeline that includes filtering out irrelevant words such as swear words, useless words, toxic sg words that hold no semantic meaning besides toxicity, and words that relate to comments performed by reddit moderator bots"],"metadata":{"id":"2oS7w6xSojNz"}},{"cell_type":"code","source":["#filter out comments that may be performed by moderators:\n","bot_words = {'bot action', 'performed automatically', 'action performed', 'submission', 'automatically', 'moderator', 'link', 'bot', 'concern', 'please'}"],"metadata":{"id":"an7JYFdTnZvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filtering function\n","def filter_comments(df, bot_words):\n","    # Create a boolean mask for comments that do not contain any of the bot words\n","    mask = ~df['text'].str.lower().str.contains('|'.join(bot_words), na=False)\n","    # Return the filtered DataFrame\n","    return df[mask]\n","\n","# Filter the DataFrame\n","df = filter_comments(negative_comments_df, bot_words)"],"metadata":{"id":"__hT-oCnndNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialise stop words list containing swear words and toxic words in sg context\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# 2. Read the words from 'swear_words.txt' and 'useless_words.txt'\n","with open('swear_words.txt', 'r') as f:\n","    swear_words = {line.strip() for line in f}\n","\n","with open('sg_toxic.txt', 'r') as f:\n","    toxic_words = {line.strip() for line in f}\n","\n","with open('useless_words.txt', 'r') as f:\n","    useless_words = {line.strip() for line in f}\n","\n","# 4. Update the stop_words set with swear words, useless words, and extra stopwords\n","stop_words.update(swear_words)\n","stop_words.update(toxic_words)\n","stop_words.update(useless_words)"],"metadata":{"id":"_dqK2gpgPVcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('wordnet')\n","lemmatizer = WordNetLemmatizer()\n","from nltk.tokenize import word_tokenize\n","\n","# Define text preprocessing function\n","def preprocess_text(text):\n","    if not isinstance(text, str):  # Check if text is a string\n","        return ''  # or handle accordingly, e.g., return None\n","    words = word_tokenize(text.lower())\n","    # Stopword removal and lemmatization\n","    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","    # Remove punctuation and non-alphabetic tokens\n","    words = [word for word in words if word.isalpha()]\n","    return ' '.join(words)"],"metadata":{"id":"49X9xiOcPXjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Vectorize text using Tf-idf\n","vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, ngram_range=(1, 2))\n","X = vectorizer.fit_transform(df['processed_comment'])"],"metadata":{"id":"ztsfXWPkPm6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##LDA for topic modelling"],"metadata":{"id":"7p2pr5i_oRp8"}},{"cell_type":"code","source":["# 5. Apply LDA Topic Modeling\n","n_topics = 10  # Number of topics\n","lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n","lda_model.fit(X)"],"metadata":{"id":"3jWrsvIJPntO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to print topics\n","def print_topics(model, vectorizer, top_n=10):\n","    words = vectorizer.get_feature_names_out()\n","    for idx, topic in enumerate(model.components_):\n","        print(f\"Topic {idx + 1}: \", [words[i] for i in topic.argsort()[-top_n:]])"],"metadata":{"id":"xEjT-3muPosJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print topics from toxic comments\n","print_topics(lda_model, vectorizer)"],"metadata":{"id":"VivDbi4FPpjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#look at coherence scores\n","texts_preprocessed = df['processed_comment']\n","\n","# Step 1: Create a Gensim Dictionary and Corpus\n","texts_tokenized = [text.split() for text in texts_preprocessed]\n","dictionary = Dictionary(texts_tokenized)\n","corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n","\n","lda = lda_model\n","# Step 2: Get the topics from the LDA model\n","lda_topics = lda.components_\n","lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n","\n","# Step 3: Calculate Coherence Score\n","coherence_model_lda = CoherenceModel(topics = lda_topics_words,\n","                                     texts = texts_tokenized,\n","                                     dictionary = dictionary,\n","                                     coherence = 'c_v')\n","\n","coherence_lda = coherence_model_lda.get_coherence()\n","print(f'Coherence Score for LDA Model: {coherence_lda}')"],"metadata":{"id":"ADUzummLPqpq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Tuning LDA Model by choosing the number of topics that give the highest coherence score"],"metadata":{"id":"PQZJmPdzoWwl"}},{"cell_type":"code","source":["#hyperparameter tuning using grid search\n","n_topics_list = [3, 5, 10, 15, 20, 25]\n","coherence_scores = []\n","\n","texts_tokenized = [text.split() for text in texts_preprocessed]\n","\n","# It should take around 15-30 seconds for each iteration\n","for n_topics in tqdm(n_topics_list):\n","\n","    lda = LatentDirichletAllocation(n_components = n_topics, random_state = 2024)\n","    lda.fit(X)\n","    lda_topics = lda.components_\n","    lda_topics_words = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_topics]\n","    coherence_model_lda = CoherenceModel(topics = lda_topics_words,\n","                                         texts = texts_tokenized,\n","                                         dictionary = dictionary,\n","                                         coherence = 'c_v')\n","    coherence_lda = coherence_model_lda.get_coherence()\n","    print(f\"Number of topics: {n_topics} | Coherence Score: {coherence_lda}\")\n","    coherence_scores.append(coherence_lda)"],"metadata":{"id":"6MzKHNfTPrqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plot coherence scores\n","import matplotlib.pyplot as plt\n","plt.plot(n_topics_list, coherence_scores)\n","plt.xlabel(\"Num Topics\")\n","plt.ylabel(\"Coherence score\")\n","plt.legend((\"coherence_values\"), loc='best')\n","plt.show()"],"metadata":{"id":"5L6NITUvPua_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#fit the model with the optimal number of topics (highest coherence score)\n","lda = LatentDirichletAllocation(n_components = 15, random_state = 2024)\n","lda.fit(X)\n","no_top_words = 10\n","tf_feature_names = vectorizer.get_feature_names_out()\n","print_topics(lda_model, vectorizer)"],"metadata":{"id":"CN9p0-fmPvQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by year and month, then perform topic modeling\n","for (year, month), group in df.groupby(['year', 'month']):\n","    print(f\"\\nYear: {year}, Month: {month}\")\n","    lda.fit(vectorizer.fit_transform(group['cleaned_text']))\n","    print_topics(lda, vectorizer.get_feature_names_out(), 10)\n"],"metadata":{"id":"Bn35qHAkPv7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## NLP Using VADER for classifying negative comments\n","\n"],"metadata":{"id":"vrOGjNBB-sb3"}},{"cell_type":"code","source":[" data_2022 = df[(df['timestamp'] >= '2022-01-01') & (df['timestamp'] < '2023-01-01')]"],"metadata":{"id":"eNP20f82_DSb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analyzer = SentimentIntensityAnalyzer()\n","data_2022['sentiment_vader'] = data_2022['text'].apply(lambda x: analyzer.polarity_scores(x)['neg'])"],"metadata":{"id":"JjdUIVmBUT2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_2022['sentiment_vader'].plot.hist()"],"metadata":{"id":"RaX_BE2ZU4__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["negative_comments = data_2022[data_2022['sentiment_vader'] > 0.5]\n","negative_comments.shape"],"metadata":{"id":"xcmTTa19Wc18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BERTopic for topic modelling"],"metadata":{"id":"ZuLVPil7WnjR"}},{"cell_type":"code","source":["topic_model = BERTopic()\n","topics, probs = topic_model.fit_transform(negative_comments['text'])"],"metadata":{"id":"JMx4TjD6X1cF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topics_overview = topic_model.get_topic_info()\n","topics_overview"],"metadata":{"id":"Eejg7-2dX7gX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_model.get_topic_info().head(7).set_index('Topic')[['Count', 'Name', 'Representation']]"],"metadata":{"id":"c0tYA1YIX-vJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_model.visualize_barchart(top_n_topics = 16, n_words = 10)"],"metadata":{"id":"gjFfSM49YDp2"},"execution_count":null,"outputs":[]}]}