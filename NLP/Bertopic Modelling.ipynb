{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Necessary Libraries"
      ],
      "metadata": {
        "id": "QNF8V3NrxDjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "import gensim\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import sklearn\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install bertopic\n",
        "from bertopic import BERTopic\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8chRnTG5mYfR",
        "outputId": "3f186ff5-a9b9-4c36-84bc-c7c42e5100a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.39-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.6)\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
            "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hdbscan-0.8.39-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, hdbscan, umap-learn, bertopic\n",
            "Successfully installed bertopic-0.16.4 hdbscan-0.8.39 pynndescent-0.5.13 umap-learn-0.5.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading datasets"
      ],
      "metadata": {
        "id": "Pj5F_G_IxMAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_2020_long = pd.read_csv('data_2020_long.csv')\n",
        "data_2022_long = pd.read_csv('data_2022_long.csv')"
      ],
      "metadata": {
        "id": "-RHm_raglQUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "rOYnmM05eC7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2020-2021 embeddings\n",
        "split_size = len(data_2020_long) // 30\n",
        "# List to store each sequential part\n",
        "sequential_splits = []\n",
        "\n",
        "for i in range(30):\n",
        "    # Calculate start and end indices for each split\n",
        "    start_idx = i * split_size\n",
        "    # Ensure the last split captures any remaining rows\n",
        "    end_idx = (i + 1) * split_size if i < 30 - 1 else len(data_2020_long)\n",
        "\n",
        "    # Slice the DataFrame\n",
        "    split_df = data_2020_long.iloc[start_idx:end_idx]\n",
        "    sequential_splits.append(split_df)"
      ],
      "metadata": {
        "id": "O6v54OobeERr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your SentenceTransformer model\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# List to hold the embeddings\n",
        "embeddings_list = []\n",
        "\n",
        "for i, part in enumerate(sequential_splits):\n",
        "    # Extract the text column (adjust column name as needed)\n",
        "    docs = part['text'].tolist()  # Replace 'text_column' with the actual column name\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = sentence_model.encode(docs)\n",
        "\n",
        "    # Store the embeddings (you can save or process them as needed)\n",
        "    embeddings_list.append(embeddings)\n",
        "\n",
        "    print(f'Processed sequential part {i+1}/{30}')"
      ],
      "metadata": {
        "id": "riXDyEJGeP9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_2020 = embeddings_list[:18]\n",
        "flattened_embeddings_2020 = np.vstack(embeddings_2020)\n",
        "len(flattened_embeddings_2020)"
      ],
      "metadata": {
        "id": "0h9EIqsweT5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"flattened_embeddings_2020.npy\", flattened_embeddings_2020)"
      ],
      "metadata": {
        "id": "Mfx-MUKieYcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2022-2023 embeddings\n",
        "split_size = len(data_2022_long) // 20"
      ],
      "metadata": {
        "id": "OCAkUtiUeiBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store each sequential part\n",
        "sequential_splits2 = []\n",
        "\n",
        "for i in range(20):\n",
        "    # Calculate start and end indices for each split\n",
        "    start_idx = i * split_size\n",
        "    # Ensure the last split captures any remaining rows\n",
        "    end_idx = (i + 1) * split_size if i < 20 - 1 else len(data_2022_long)\n",
        "\n",
        "    # Slice the DataFrame\n",
        "    split_df = data_2022_long.iloc[start_idx:end_idx]\n",
        "    sequential_splits2.append(split_df)"
      ],
      "metadata": {
        "id": "7J5w_28JesVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your SentenceTransformer model\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# List to hold the embeddings\n",
        "embeddings_list_2 = []\n",
        "\n",
        "for i, part in enumerate(sequential_splits2):\n",
        "    # Extract the text column (adjust column name as needed)\n",
        "    docs = part['text'].tolist()  # Replace 'text_column' with the actual column name\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = sentence_model.encode(docs)\n",
        "\n",
        "    # Store the embeddings (you can save or process them as needed)\n",
        "    embeddings_list_2.append(embeddings)\n",
        "\n",
        "    print(f'Processed sequential part {i+1}/{20}')"
      ],
      "metadata": {
        "id": "hrOS5RSne0pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_2022 = embeddings_list_2\n",
        "flattened_embeddings_2022 = np.vstack(embeddings_2022)\n",
        "len(flattened_embeddings_2022)"
      ],
      "metadata": {
        "id": "0TwH81xje31e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save as .npy file\n",
        "np.save(\"flattened_embeddings_2022.npy\", flattened_embeddings_2022)"
      ],
      "metadata": {
        "id": "PQ4SVdHYe_rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topic Modelling for 2023 comments"
      ],
      "metadata": {
        "id": "ueLtPDjSxjoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for observations in 2023\n",
        "filtered_df = data_2022_long[data_2022_long['timestamp'].dt.year == 2023]\n",
        "\n",
        "# Get the indices of the filtered DataFrame\n",
        "filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "# Extract the corresponding embeddings using the filtered indices\n",
        "filtered_embeddings = [flattened_embeddings_2022[i] for i in filtered_indices]\n",
        "\n",
        "\n",
        "# Verify truth\n",
        "len(filtered_df) == len(filtered_embeddings)"
      ],
      "metadata": {
        "id": "iIqjili1gGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "#Best parameters\n",
        "best_hdbscan = HDBSCAN(cluster_selection_method='eom', metric='euclidean',\n",
        "        min_cluster_size=300, min_samples=5)\n",
        "\n",
        "#Best bertopic model\n",
        "topic_model = BERTopic(hdbscan_model=best_hdbscan)\n",
        "\n",
        "#fit topic modelling to the preprocessed text data\n",
        "topics, probabilities = topic_model.fit_transform(filtered_df[\"text\"], np.array(filtered_embeddings))"
      ],
      "metadata": {
        "id": "JptLa3s_gI2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
        "\n",
        "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
        "\n",
        "top_15_topics"
      ],
      "metadata": {
        "id": "ifihmOEagixA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics['Count'].sum()/len(filtered_df)"
      ],
      "metadata": {
        "id": "vBUIEO5_g3pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign each comment to a topic\n",
        "filtered_df['topic'] = topics"
      ],
      "metadata": {
        "id": "R6N70qkBg7sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
        "\n",
        "data_top15.head(5)"
      ],
      "metadata": {
        "id": "GoOtK1a3g-zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15.to_csv(\"output/2023_comments_top15topics.csv\")\n",
        "top_15_topics.to_csv(\"output/2023top15topics.csv\")"
      ],
      "metadata": {
        "id": "Xe8MyhgMhU3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling for 2022 comments"
      ],
      "metadata": {
        "id": "QNV5C5w7hw5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for observations in 2022\n",
        "filtered_df = data_2022_long[data_2022_long['timestamp'].dt.year == 2022]\n",
        "\n",
        "# Get the indices of the filtered DataFrame\n",
        "filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "# Extract the corresponding embeddings using the filtered indices\n",
        "filtered_embeddings = [flattened_embeddings_2022[i] for i in filtered_indices]\n",
        "\n",
        "\n",
        "# Verify truth\n",
        "len(filtered_df) == len(filtered_embeddings)"
      ],
      "metadata": {
        "id": "yhRdQi5mh2Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
        "\n",
        "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
        "\n",
        "top_15_topics"
      ],
      "metadata": {
        "id": "8C5EKq93h5Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics['Count'].sum()/len(filtered_df)"
      ],
      "metadata": {
        "id": "2adkJ5cKh7h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign each comment to a topic\n",
        "filtered_df['topic'] = topics"
      ],
      "metadata": {
        "id": "O9x9YGr5h89q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
        "\n",
        "data_top15.head(5)"
      ],
      "metadata": {
        "id": "-6LVBgfph-Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15.to_csv(\"output/2022_comments_top15topics.csv\")\n",
        "top_15_topics.to_csv(\"output/2022top15topics.csv\")"
      ],
      "metadata": {
        "id": "5uD5_RIRh_ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling for 2021 comments"
      ],
      "metadata": {
        "id": "6_ky8e4miIKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for observations in 2022\n",
        "filtered_df = data_2020_long[data_2020_long['timestamp'].dt.year == 2021]\n",
        "\n",
        "# Get the indices of the filtered DataFrame\n",
        "filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "# Extract the corresponding embeddings using the filtered indices\n",
        "filtered_embeddings = [flattened_embeddings_2020[i] for i in filtered_indices]\n",
        "\n",
        "\n",
        "# Verify truth\n",
        "len(filtered_df) == len(filtered_embeddings)"
      ],
      "metadata": {
        "id": "CuGE5Vw0iLCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
        "\n",
        "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
        "\n",
        "top_15_topics"
      ],
      "metadata": {
        "id": "IOScpO0eiPUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics['Count'].sum()/len(filtered_df)"
      ],
      "metadata": {
        "id": "-Ej-O_ViiQ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign each comment to a topic\n",
        "filtered_df['topic'] = topics"
      ],
      "metadata": {
        "id": "6OxSeCRWiTjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
        "\n",
        "data_top15.head(5)"
      ],
      "metadata": {
        "id": "JxLTAHX4iU0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15.to_csv(\"output/2021_comments_top15topics.csv\")\n",
        "top_15_topics.to_csv(\"output/2021top15topics.csv\")"
      ],
      "metadata": {
        "id": "OMvl6x_aiWG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling for 2020 comments"
      ],
      "metadata": {
        "id": "l5Izgt4eiZJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for observations in 2022\n",
        "filtered_df = data_2020_long[data_2020_long['timestamp'].dt.year == 2020]\n",
        "\n",
        "# Get the indices of the filtered DataFrame\n",
        "filtered_indices = filtered_df.index.tolist()\n",
        "\n",
        "# Extract the corresponding embeddings using the filtered indices\n",
        "filtered_embeddings = [flattened_embeddings_2020[i] for i in filtered_indices]\n",
        "\n",
        "\n",
        "# Verify truth\n",
        "len(filtered_df) == len(filtered_embeddings)"
      ],
      "metadata": {
        "id": "A1arHfDKicEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
        "\n",
        "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
        "\n",
        "top_15_topics"
      ],
      "metadata": {
        "id": "JUYkRZ_hiemb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_15_topics['Count'].sum()/len(filtered_df)"
      ],
      "metadata": {
        "id": "tsRwAYRoifxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign each comment to a topic\n",
        "filtered_df['topic'] = topics"
      ],
      "metadata": {
        "id": "S5wiaSt0ihZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
        "\n",
        "data_top15.head(5)"
      ],
      "metadata": {
        "id": "evc1CGHriizs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_top15.to_csv(\"output/2020_comments_top15topics.csv\")\n",
        "top_15_topics.to_csv(\"output/2020top15topics.csv\")"
      ],
      "metadata": {
        "id": "qogLNcsGij58"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}