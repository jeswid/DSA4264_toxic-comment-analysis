{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QNF8V3NrxDjK"
   },
   "source": [
    "## Loading Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8chRnTG5mYfR",
    "outputId": "3f186ff5-a9b9-4c36-84bc-c7c42e5100a3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj5F_G_IxMAJ"
   },
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-RHm_raglQUo"
   },
   "outputs": [],
   "source": [
    "data_2020_long = pd.read_csv('data_2020_long.csv')\n",
    "data_2022_long = pd.read_csv('data_2022_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020_long['timestamp'] = pd.to_datetime(data_2020_long['timestamp'], errors='coerce')\n",
    "data_2022_long['timestamp'] = pd.to_datetime(data_2022_long['timestamp'], errors='coerce')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rOYnmM05eC7z"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6v54OobeERr"
   },
   "outputs": [],
   "source": [
    "## 2020-2021 embeddings\n",
    "split_size = len(data_2020_long) // 30\n",
    "# List to store each sequential part\n",
    "sequential_splits = []\n",
    "\n",
    "for i in range(30):\n",
    "    # Calculate start and end indices for each split\n",
    "    start_idx = i * split_size\n",
    "    # Ensure the last split captures any remaining rows\n",
    "    end_idx = (i + 1) * split_size if i < 30 - 1 else len(data_2020_long)\n",
    "\n",
    "    # Slice the DataFrame\n",
    "    split_df = data_2020_long.iloc[start_idx:end_idx]\n",
    "    sequential_splits.append(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riXDyEJGeP9q"
   },
   "outputs": [],
   "source": [
    "# Initialize your SentenceTransformer model\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# List to hold the embeddings\n",
    "embeddings_list = []\n",
    "\n",
    "for i, part in enumerate(sequential_splits):\n",
    "    # Extract the text column (adjust column name as needed)\n",
    "    docs = part['text'].tolist()  # Replace 'text_column' with the actual column name\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = sentence_model.encode(docs)\n",
    "\n",
    "    # Store the embeddings (you can save or process them as needed)\n",
    "    embeddings_list.append(embeddings)\n",
    "\n",
    "    print(f'Processed sequential part {i+1}/{30}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h9EIqsweT5O"
   },
   "outputs": [],
   "source": [
    "embeddings_2020 = embeddings_list[:18]\n",
    "flattened_embeddings_2020 = np.vstack(embeddings_2020)\n",
    "len(flattened_embeddings_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfx-MUKieYcu"
   },
   "outputs": [],
   "source": [
    "np.save(\"flattened_embeddings_2020.npy\", flattened_embeddings_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCAkUtiUeiBt"
   },
   "outputs": [],
   "source": [
    "## 2022-2023 embeddings\n",
    "split_size = len(data_2022_long) // 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J5w_28JesVS"
   },
   "outputs": [],
   "source": [
    "# List to store each sequential part\n",
    "sequential_splits2 = []\n",
    "\n",
    "for i in range(20):\n",
    "    # Calculate start and end indices for each split\n",
    "    start_idx = i * split_size\n",
    "    # Ensure the last split captures any remaining rows\n",
    "    end_idx = (i + 1) * split_size if i < 20 - 1 else len(data_2022_long)\n",
    "\n",
    "    # Slice the DataFrame\n",
    "    split_df = data_2022_long.iloc[start_idx:end_idx]\n",
    "    sequential_splits2.append(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrOS5RSne0pl"
   },
   "outputs": [],
   "source": [
    "# Initialize your SentenceTransformer model\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# List to hold the embeddings\n",
    "embeddings_list_2 = []\n",
    "\n",
    "for i, part in enumerate(sequential_splits2):\n",
    "    # Extract the text column (adjust column name as needed)\n",
    "    docs = part['text'].tolist()  # Replace 'text_column' with the actual column name\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = sentence_model.encode(docs)\n",
    "\n",
    "    # Store the embeddings (you can save or process them as needed)\n",
    "    embeddings_list_2.append(embeddings)\n",
    "\n",
    "    print(f'Processed sequential part {i+1}/{20}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TwH81xje31e"
   },
   "outputs": [],
   "source": [
    "embeddings_2022 = embeddings_list_2\n",
    "flattened_embeddings_2022 = np.vstack(embeddings_2022)\n",
    "len(flattened_embeddings_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ4SVdHYe_rB"
   },
   "outputs": [],
   "source": [
    "# Save as .npy file\n",
    "np.save(\"flattened_embeddings_2022.npy\", flattened_embeddings_2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_embeddings_2020 = np.load(\"flattened_embeddings_2020.npy\")\n",
    "flattened_embeddings_2022 = np.load(\"flattened_embeddings_2022.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ueLtPDjSxjoE"
   },
   "source": [
    "## Topic Modelling for 2023 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIqjili1gGNe"
   },
   "outputs": [],
   "source": [
    "# Filter for observations in 2023\n",
    "filtered_df = data_2022_long[data_2022_long['timestamp'].dt.year == 2023]\n",
    "\n",
    "# Get the indices of the filtered DataFrame\n",
    "filtered_indices = filtered_df.index.tolist()\n",
    "\n",
    "# Extract the corresponding embeddings using the filtered indices\n",
    "filtered_embeddings = [flattened_embeddings_2022[i] for i in filtered_indices]\n",
    "\n",
    "\n",
    "# Verify truth\n",
    "len(filtered_df) == len(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JptLa3s_gI2_"
   },
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "best_hdbscan = HDBSCAN(cluster_selection_method='eom', metric='euclidean',\n",
    "        min_cluster_size=300, min_samples=5)\n",
    "\n",
    "#Best bertopic model\n",
    "topic_model = BERTopic(hdbscan_model=best_hdbscan)\n",
    "\n",
    "#fit topic modelling to the preprocessed text data\n",
    "topics, probabilities = topic_model.fit_transform(filtered_df[\"text\"], np.array(filtered_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifihmOEagixA"
   },
   "outputs": [],
   "source": [
    "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
    "\n",
    "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
    "\n",
    "top_15_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBUIEO5_g3pQ"
   },
   "outputs": [],
   "source": [
    "top_15_topics['Count'].sum()/len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6N70qkBg7sg"
   },
   "outputs": [],
   "source": [
    "#Assign each comment to a topic\n",
    "filtered_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoOtK1a3g-zF"
   },
   "outputs": [],
   "source": [
    "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
    "\n",
    "data_top15.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe8MyhgMhU3c"
   },
   "outputs": [],
   "source": [
    "data_top15.to_csv(\"output/2023_comments_top15topics.csv\")\n",
    "top_15_topics.to_csv(\"output/2023top15topics.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QNV5C5w7hw5q"
   },
   "source": [
    "## Topic Modelling for 2022 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhRdQi5mh2Dn"
   },
   "outputs": [],
   "source": [
    "# Filter for observations in 2022\n",
    "filtered_df = data_2022_long[data_2022_long['timestamp'].dt.year == 2022]\n",
    "\n",
    "# Get the indices of the filtered DataFrame\n",
    "filtered_indices = filtered_df.index.tolist()\n",
    "\n",
    "# Extract the corresponding embeddings using the filtered indices\n",
    "filtered_embeddings = [flattened_embeddings_2022[i] for i in filtered_indices]\n",
    "\n",
    "\n",
    "# Verify truth\n",
    "len(filtered_df) == len(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "best_hdbscan = HDBSCAN(cluster_selection_method='eom', metric='euclidean',\n",
    "        min_cluster_size=300, min_samples=5)\n",
    "\n",
    "#Best bertopic model\n",
    "topic_model = BERTopic(hdbscan_model=best_hdbscan)\n",
    "\n",
    "#fit topic modelling to the preprocessed text data\n",
    "topics, probabilities = topic_model.fit_transform(filtered_df[\"text\"], np.array(filtered_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8C5EKq93h5Bf"
   },
   "outputs": [],
   "source": [
    "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
    "\n",
    "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
    "\n",
    "top_15_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2adkJ5cKh7h6"
   },
   "outputs": [],
   "source": [
    "top_15_topics['Count'].sum()/len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9x9YGr5h89q"
   },
   "outputs": [],
   "source": [
    "#Assign each comment to a topic\n",
    "filtered_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6LVBgfph-Qb"
   },
   "outputs": [],
   "source": [
    "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
    "\n",
    "data_top15.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uD5_RIRh_ce"
   },
   "outputs": [],
   "source": [
    "data_top15.to_csv(\"output/2022_comments_top15topics.csv\")\n",
    "top_15_topics.to_csv(\"output/2022top15topics.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6_ky8e4miIKW"
   },
   "source": [
    "## Topic Modelling for 2021 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CuGE5Vw0iLCf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for observations in 2022\n",
    "filtered_df = data_2020_long[data_2020_long['timestamp'].dt.year == 2021]\n",
    "\n",
    "# Get the indices of the filtered DataFrame\n",
    "filtered_indices = filtered_df.index.tolist()\n",
    "\n",
    "# Extract the corresponding embeddings using the filtered indices\n",
    "filtered_embeddings = [flattened_embeddings_2020[i] for i in filtered_indices]\n",
    "\n",
    "\n",
    "# Verify truth\n",
    "len(filtered_df) == len(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters\n",
    "best_hdbscan = HDBSCAN(cluster_selection_method='eom', metric='euclidean',\n",
    "        min_cluster_size=300, min_samples=5)\n",
    "\n",
    "#Best bertopic model\n",
    "topic_model = BERTopic(hdbscan_model=best_hdbscan)\n",
    "\n",
    "#fit topic modelling to the preprocessed text data\n",
    "topics, probabilities = topic_model.fit_transform(filtered_df[\"text\"], np.array(filtered_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOScpO0eiPUp"
   },
   "outputs": [],
   "source": [
    "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
    "\n",
    "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
    "\n",
    "top_15_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ej-O_ViiQ4m"
   },
   "outputs": [],
   "source": [
    "top_15_topics['Count'].sum()/len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OxSeCRWiTjm"
   },
   "outputs": [],
   "source": [
    "#Assign each comment to a topic\n",
    "filtered_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxLTAHX4iU0r"
   },
   "outputs": [],
   "source": [
    "data_top15 = filtered_df[(filtered_df['topic'] < 15) & (filtered_df['topic'] >= 0)]\n",
    "\n",
    "data_top15.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMvl6x_aiWG5"
   },
   "outputs": [],
   "source": [
    "data_top15.to_csv(\"output/2021_comments_top15topics.csv\")\n",
    "top_15_topics.to_csv(\"output/2021top15topics.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Izgt4eiZJo"
   },
   "source": [
    "## Topic Modelling for 2020 comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A1arHfDKicEV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for observations in 2022\n",
    "filtered_df = data_2020_long[data_2020_long['timestamp'].dt.year == 2020]\n",
    "\n",
    "# Get the indices of the filtered DataFrame\n",
    "filtered_indices = filtered_df.index.tolist()\n",
    "\n",
    "# Extract the corresponding embeddings using the filtered indices\n",
    "filtered_embeddings = [flattened_embeddings_2020[i] for i in filtered_indices]\n",
    "\n",
    "\n",
    "# Verify truth\n",
    "len(filtered_df) == len(filtered_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "#Best parameters\n",
    "best_hdbscan = HDBSCAN(cluster_selection_method='eom', metric='euclidean',\n",
    "        min_cluster_size=300, min_samples=5)\n",
    "\n",
    "#Best bertopic model\n",
    "topic_model = BERTopic(hdbscan_model=best_hdbscan)\n",
    "\n",
    "#fit topic modelling to the preprocessed text data\n",
    "topics, probabilities = topic_model.fit_transform(filtered_df[\"text\"], np.array(filtered_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
    "\n",
    "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
    "\n",
    "top_15_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tsRwAYRoifxT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13673076204823026"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_15_topics['Count'].sum()/len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S5wiaSt0ihZw"
   },
   "outputs": [],
   "source": [
    "#Assign each comment to a topic\n",
    "filtered_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_topics = topic_model.get_topic_info().head(16).set_index('Topic')[['Count', 'Name', 'Representation']]\n",
    "\n",
    "top_15_topics = top_15_topics.drop(index=-1, errors='ignore')\n",
    "\n",
    "top_15_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qogLNcsGij58"
   },
   "outputs": [],
   "source": [
    "data_top15.to_csv(\"output/2020_comments_top15topics.csv\")\n",
    "top_15_topics.to_csv(\"output/2020top15topics.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
