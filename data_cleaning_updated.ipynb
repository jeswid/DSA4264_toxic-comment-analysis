{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eushae-\n",
      "[nltk_data]     anne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "reddit_df = pd.read_csv(\"Reddit-Threads_2022-2023.csv\")\n",
    "reddit_df['original text'] = reddit_df['text'].copy()\n",
    "\n",
    "# remove stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop words include: a, about, above, after, again, against, all, am, an, and, any, are, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can't, cannot, could, couldn't, did, didn't, do, does, doesn't, doing, don't, down, during, each, few, for, from, further, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, he's, her, here, here's, hers, herself, him, himself, his, how, how's, i, i'd, i'll, i'm, i've, if, in, into, is, isn't, it, it's, its, itself, let's, me, more, most, mustn't, my, myself, no, nor, not, of, off, on, once, only, or, other, ought, our, ours, ourselves, out, over, own, same, shan't, she, she'd, she'll, she's, should, shouldn't, so, some, such, than, that, that's, the, their, theirs, them, themselves, then, there, there's, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, very, was, wasn't, we, we'd, we'll, we're, we've, were, weren't, what, what's, when, when's, where, where's, which, while, who, who's, whom, why, why's, with, won't, would, wouldn't, you, you'd, you'll, you're, you've, your, yours, yourself, yourselves\n",
    "\n",
    "#add Singlish stop words\n",
    "singlish_stopwords = {'ah', 'ah beng', 'ah boh', 'ahbeng', 'ahlian', 'ahyo', 'aiya', 'aiyo', 'aiyoh', 'alamak', 'also', 'already', \n",
    "    'ang moh', 'angkat', 'anyhow', 'arbo', 'atas', 'bah', 'balik', 'bao', 'bo', 'bochap', 'boh eng', 'boh liao', \n",
    "    'boleh', 'bro', 'brudder', 'bruh', 'buay', 'can', 'can lah', 'chao', 'cheem', 'chey', 'chiobu', 'chio', \n",
    "    'chiong', 'chop chop', 'dai ji', 'den', 'dude', 'dun', 'eh', 'ehh', 'even', 'gao', 'got meh', 'guai', \n",
    "    'hao lian', 'heng', 'hor', 'huh', 'is it', 'issit', 'izzit', 'jalan jalan', 'jialat', 'jin', 'just', 'kena', \n",
    "    'kiasi', 'keke', 'kena', 'kopi', 'kwai', 'la', 'la kopi', 'lah', 'lahh', 'lahz', 'laks', 'liao', 'liaoz', \n",
    "    'liaozz', 'like', 'lor', 'lor lor', 'lorh', 'lorz', 'mai', 'makan', 'makan makan', 'machiam', 'mah', 'malu', \n",
    "    'maybe', 'meh', 'nah', 'nia', 'no lah', 'nope', 'not', 'ok', 'ok lah', 'okie', 'omg', 'onz', 'pai seh', \n",
    "    'paiseh', 'really', 'relak', 'right', 'sial', 'sialz', 'sia lah', 'siaoz', 'sian', 'sianz', 'shiok', \n",
    "    'shiok lah', 'simi', 'sis', 'steady', 'still', 'suka', 'suka suka', 'suay', 'tsk', 'tahan', 'tapao', \n",
    "    'terok', 'then', 'tink', 'tio', 'tok', 'true', 'wah', 'wah lao', 'wah piang', 'wahhh', 'waseh', 'what', \n",
    "    'when', 'where', 'who', 'yah', 'yet', 'yeap', 'yea', 'yup', 'zai', 'zhun'\n",
    "}\n",
    "\n",
    "stop_words.update(singlish_stopwords)\n",
    "\n",
    "# To lowercase\n",
    "reddit_df['text'] = reddit_df['text'].str.lower()\n",
    "\n",
    "# Remove punctuation using str.replace() and string.punctuation\n",
    "reddit_df['text'] = reddit_df['text'].str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "\n",
    "# Convert all values to strings, replace NaNs with an empty string\n",
    "reddit_df['text'] = reddit_df['text'].fillna('').astype(str)\n",
    "reddit_df['text'] = reddit_df['text'].apply(lambda x: ' '.join(word for word in x.split() if word.lower() not in stop_words))\n",
    "\n",
    "# remove chinese characters\n",
    "reddit_df['text'] = reddit_df['text'].str.replace(r'[\\u4e00-\\u9fff]+', '', regex=True)\n",
    "\n",
    "# Remove '&gt;' from the 'text' column\n",
    "reddit_df['text'] = reddit_df['text'].str.replace('&gt;', '', regex=False)\n",
    "\n",
    "# remove emoji and other characters\n",
    "def remove_non_ascii(text):\n",
    "    # Keep only ASCII characters (characters with codes between 0 and 127)\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "reddit_df['text'] = reddit_df['text'].astype(str).apply(remove_non_ascii)\n",
    "\n",
    "# remove '[deleted]', '.', '[removed]', ''\n",
    "toremove = ['deleted', 'removed', '']\n",
    "reddit_df = reddit_df[~reddit_df['text'].isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file\n",
    "reddit_df.to_csv(\"cleaned_data_2223.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for chunk in pd.read_csv(\"Reddit-Threads_2020-2021.csv\", engine='python', chunksize=1000, on_bad_lines='skip'):\n",
    "  chunks.append(chunk)\n",
    "\n",
    "reddit_df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eushae-\n",
      "[nltk_data]     anne/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "reddit_df['original text'] = reddit_df['text'].copy()\n",
    "\n",
    "# remove stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop words include: a, about, above, after, again, against, all, am, an, and, any, are, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can't, cannot, could, couldn't, did, didn't, do, does, doesn't, doing, don't, down, during, each, few, for, from, further, had, hadn't, has, hasn't, have, haven't, having, he, he'd, he'll, he's, her, here, here's, hers, herself, him, himself, his, how, how's, i, i'd, i'll, i'm, i've, if, in, into, is, isn't, it, it's, its, itself, let's, me, more, most, mustn't, my, myself, no, nor, not, of, off, on, once, only, or, other, ought, our, ours, ourselves, out, over, own, same, shan't, she, she'd, she'll, she's, should, shouldn't, so, some, such, than, that, that's, the, their, theirs, them, themselves, then, there, there's, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, very, was, wasn't, we, we'd, we'll, we're, we've, were, weren't, what, what's, when, when's, where, where's, which, while, who, who's, whom, why, why's, with, won't, would, wouldn't, you, you'd, you'll, you're, you've, your, yours, yourself, yourselves\n",
    "\n",
    "#add Singlish stop words\n",
    "singlish_stopwords = {'ah', 'ah beng', 'ah boh', 'ahbeng', 'ahlian', 'ahyo', 'aiya', 'aiyo', 'aiyoh', 'alamak', 'also', 'already', \n",
    "    'ang moh', 'angkat', 'anyhow', 'arbo', 'atas', 'bah', 'balik', 'bao', 'bo', 'bochap', 'boh eng', 'boh liao', \n",
    "    'boleh', 'bro', 'brudder', 'bruh', 'buay', 'can', 'can lah', 'chao', 'cheem', 'chey', 'chiobu', 'chio', \n",
    "    'chiong', 'chop chop', 'dai ji', 'den', 'dude', 'dun', 'eh', 'ehh', 'even', 'gao', 'got meh', 'guai', \n",
    "    'hao lian', 'heng', 'hor', 'huh', 'is it', 'issit', 'izzit', 'jalan jalan', 'jialat', 'jin', 'just', 'kena', \n",
    "    'kiasi', 'keke', 'kena', 'kopi', 'kwai', 'la', 'la kopi', 'lah', 'lahh', 'lahz', 'laks', 'liao', 'liaoz', \n",
    "    'liaozz', 'like', 'lor', 'lor lor', 'lorh', 'lorz', 'mai', 'makan', 'makan makan', 'machiam', 'mah', 'malu', \n",
    "    'maybe', 'meh', 'nah', 'nia', 'no lah', 'nope', 'not', 'ok', 'ok lah', 'okie', 'omg', 'onz', 'pai seh', \n",
    "    'paiseh', 'really', 'relak', 'right', 'sial', 'sialz', 'sia lah', 'siaoz', 'sian', 'sianz', 'shiok', \n",
    "    'shiok lah', 'simi', 'sis', 'steady', 'still', 'suka', 'suka suka', 'suay', 'tsk', 'tahan', 'tapao', \n",
    "    'terok', 'then', 'tink', 'tio', 'tok', 'true', 'wah', 'wah lao', 'wah piang', 'wahhh', 'waseh', 'what', \n",
    "    'when', 'where', 'who', 'yah', 'yet', 'yeap', 'yea', 'yup', 'zai', 'zhun'\n",
    "}\n",
    "\n",
    "stop_words.update(singlish_stopwords)\n",
    "\n",
    "# To lowercase\n",
    "reddit_df['text'] = reddit_df['text'].str.lower()\n",
    "\n",
    "# Remove punctuation using str.replace() and string.punctuation\n",
    "reddit_df['text'] = reddit_df['text'].str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "\n",
    "# Convert all values to strings, replace NaNs with an empty string\n",
    "reddit_df['text'] = reddit_df['text'].fillna('').astype(str)\n",
    "reddit_df['text'] = reddit_df['text'].apply(lambda x: ' '.join(word for word in x.split() if word.lower() not in stop_words))\n",
    "\n",
    "# remove chinese characters\n",
    "reddit_df['text'] = reddit_df['text'].str.replace(r'[\\u4e00-\\u9fff]+', '', regex=True)\n",
    "\n",
    "# Remove '&gt;' from the 'text' column\n",
    "reddit_df['text'] = reddit_df['text'].str.replace('&gt;', '', regex=False)\n",
    "\n",
    "# remove emoji and other characters\n",
    "def remove_non_ascii(text):\n",
    "    # Keep only ASCII characters (characters with codes between 0 and 127)\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "reddit_df['text'] = reddit_df['text'].astype(str).apply(remove_non_ascii)\n",
    "\n",
    "# remove '[deleted]', '.', '[removed]', ''\n",
    "toremove = ['deleted', 'removed', '']\n",
    "reddit_df = reddit_df[~reddit_df['text'].isin(toremove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file\n",
    "reddit_df.to_csv(\"cleaned_data_2021.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for comments longer than 8 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020 = pd.read_csv(\"cleaned_data_2021.csv\", index_col=0)\n",
    "data_2020['timestamp'] = pd.to_datetime(data_2020['timestamp'])\n",
    "data_2020['comment_length'] = data_2020['text'].str.split(' ').str.len()\n",
    "data_2020['text'] = data_2020['text'].astype(str)\n",
    "\n",
    "data_2020_long = data_2020[data_2020['comment_length'] > 8]\n",
    "\n",
    "data_2020_long.to_csv(\"data_2020_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2022 = pd.read_csv(\"cleaned_data_2223.csv\", index_col=0)\n",
    "data_2022['timestamp'] = pd.to_datetime(data_2022['timestamp'])\n",
    "data_2022['comment_length'] = data_2022['text'].str.split(' ').str.len()\n",
    "data_2022['text'] = data_2022['text'].astype(str)\n",
    "\n",
    "data_2022_long = data_2022[data_2022['comment_length'] > 8]\n",
    "\n",
    "data_2022_long.to_csv(\"data_2022_long.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
